{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cpu\n",
      "Loading data...\n",
      "Preprocessing data...\n",
      "Training model...\n",
      "Epoch 1, Train Loss: 0.6184, Val Loss: 0.4393\n",
      "Epoch 2, Train Loss: 0.2418, Val Loss: 0.1408\n",
      "Epoch 3, Train Loss: 0.1164, Val Loss: 0.0715\n",
      "Epoch 4, Train Loss: 0.0729, Val Loss: 0.0357\n",
      "Epoch 5, Train Loss: 0.0587, Val Loss: 0.0275\n",
      "Epoch 6, Train Loss: 0.0450, Val Loss: 0.0186\n",
      "Epoch 7, Train Loss: 0.0339, Val Loss: 0.0187\n",
      "Epoch 8, Train Loss: 0.0309, Val Loss: 0.0132\n",
      "Epoch 9, Train Loss: 0.0226, Val Loss: 0.0091\n",
      "Epoch 10, Train Loss: 0.0168, Val Loss: 0.0047\n",
      "Epoch 11, Train Loss: 0.0214, Val Loss: 0.0038\n",
      "Epoch 12, Train Loss: 0.0143, Val Loss: 0.0028\n",
      "Epoch 13, Train Loss: 0.0160, Val Loss: 0.0024\n",
      "Epoch 14, Train Loss: 0.0130, Val Loss: 0.0032\n",
      "Epoch 15, Train Loss: 0.0064, Val Loss: 0.0058\n",
      "Epoch 16, Train Loss: 0.0155, Val Loss: 0.0054\n",
      "Epoch 17, Train Loss: 0.0103, Val Loss: 0.0009\n",
      "Epoch 18, Train Loss: 0.0084, Val Loss: 0.0035\n",
      "Epoch 19, Train Loss: 0.0074, Val Loss: 0.0010\n",
      "Epoch 20, Train Loss: 0.0055, Val Loss: 0.0006\n",
      "Epoch 21, Train Loss: 0.0086, Val Loss: 0.0006\n",
      "Epoch 22, Train Loss: 0.0061, Val Loss: 0.0003\n",
      "Epoch 23, Train Loss: 0.0043, Val Loss: 0.0004\n",
      "Epoch 24, Train Loss: 0.0027, Val Loss: 0.0003\n",
      "Epoch 25, Train Loss: 0.0019, Val Loss: 0.0002\n",
      "Epoch 26, Train Loss: 0.0037, Val Loss: 0.0001\n",
      "Epoch 27, Train Loss: 0.0024, Val Loss: 0.0001\n",
      "Epoch 28, Train Loss: 0.0021, Val Loss: 0.0001\n",
      "Epoch 29, Train Loss: 0.0086, Val Loss: 0.0004\n",
      "Epoch 30, Train Loss: 0.0052, Val Loss: 0.0007\n",
      "Epoch 31, Train Loss: 0.0039, Val Loss: 0.0001\n",
      "Epoch 32, Train Loss: 0.0033, Val Loss: 0.0001\n",
      "Epoch 33, Train Loss: 0.0018, Val Loss: 0.0001\n",
      "Epoch 34, Train Loss: 0.0016, Val Loss: 0.0001\n",
      "Epoch 35, Train Loss: 0.0018, Val Loss: 0.0001\n",
      "Epoch 36, Train Loss: 0.0019, Val Loss: 0.0001\n",
      "Epoch 37, Train Loss: 0.0019, Val Loss: 0.0001\n",
      "Epoch 38, Train Loss: 0.0017, Val Loss: 0.0001\n",
      "Epoch 39, Train Loss: 0.0013, Val Loss: 0.0001\n",
      "Epoch 40, Train Loss: 0.0015, Val Loss: 0.0001\n",
      "Epoch 41, Train Loss: 0.0015, Val Loss: 0.0001\n",
      "Epoch 42, Train Loss: 0.0025, Val Loss: 0.0001\n",
      "Epoch 43, Train Loss: 0.0016, Val Loss: 0.0001\n",
      "Epoch 44, Train Loss: 0.0015, Val Loss: 0.0001\n",
      "Epoch 45, Train Loss: 0.0020, Val Loss: 0.0001\n",
      "Epoch 46, Train Loss: 0.0018, Val Loss: 0.0001\n",
      "Epoch 47, Train Loss: 0.0015, Val Loss: 0.0001\n",
      "Epoch 48, Train Loss: 0.0032, Val Loss: 0.0001\n",
      "Epoch 49, Train Loss: 0.0039, Val Loss: 0.0001\n",
      "Early stopping triggered after 49 epochs\n",
      "Done training, time: 65.31 秒\n",
      "在验证集上寻找最佳阈值...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2257642/1044010578.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_gru_lstm_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在测试集上进行最终评估...\n",
      "Loading data...\n",
      "Preprocessing data...\n",
      "Training model...\n",
      "Epoch 1, Train Loss: 0.5834, Val Loss: 0.2983\n",
      "Epoch 2, Train Loss: 0.1861, Val Loss: 0.1240\n",
      "Epoch 3, Train Loss: 0.0811, Val Loss: 0.0639\n",
      "Epoch 4, Train Loss: 0.0317, Val Loss: 0.0496\n",
      "Epoch 5, Train Loss: 0.0263, Val Loss: 0.0431\n",
      "Epoch 6, Train Loss: 0.0187, Val Loss: 0.0458\n",
      "Epoch 7, Train Loss: 0.0160, Val Loss: 0.0560\n",
      "Epoch 8, Train Loss: 0.0165, Val Loss: 0.0388\n",
      "Epoch 9, Train Loss: 0.0122, Val Loss: 0.0480\n",
      "Epoch 10, Train Loss: 0.0142, Val Loss: 0.0397\n",
      "Epoch 11, Train Loss: 0.0100, Val Loss: 0.0438\n",
      "Epoch 12, Train Loss: 0.0082, Val Loss: 0.0408\n",
      "Epoch 13, Train Loss: 0.0060, Val Loss: 0.0408\n",
      "Epoch 14, Train Loss: 0.0058, Val Loss: 0.0409\n",
      "Epoch 15, Train Loss: 0.0065, Val Loss: 0.0411\n",
      "Epoch 16, Train Loss: 0.0051, Val Loss: 0.0411\n",
      "Epoch 17, Train Loss: 0.0029, Val Loss: 0.0412\n",
      "Epoch 18, Train Loss: 0.0035, Val Loss: 0.0412\n",
      "Early stopping triggered after 18 epochs\n",
      "Done training, time: 24.38 秒\n",
      "在验证集上寻找最佳阈值...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2257642/1044010578.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_gru_lstm_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在测试集上进行最终评估...\n",
      "Loading data...\n",
      "Preprocessing data...\n",
      "Training model...\n",
      "Epoch 1, Train Loss: 0.6162, Val Loss: 0.4313\n",
      "Epoch 2, Train Loss: 0.2713, Val Loss: 0.1557\n",
      "Epoch 3, Train Loss: 0.1458, Val Loss: 0.1113\n",
      "Epoch 4, Train Loss: 0.1018, Val Loss: 0.0715\n",
      "Epoch 5, Train Loss: 0.0826, Val Loss: 0.0586\n",
      "Epoch 6, Train Loss: 0.0676, Val Loss: 0.0425\n",
      "Epoch 7, Train Loss: 0.0631, Val Loss: 0.0373\n",
      "Epoch 8, Train Loss: 0.0487, Val Loss: 0.0324\n",
      "Epoch 9, Train Loss: 0.0397, Val Loss: 0.0325\n",
      "Epoch 10, Train Loss: 0.0389, Val Loss: 0.0225\n",
      "Epoch 11, Train Loss: 0.0335, Val Loss: 0.0134\n",
      "Epoch 12, Train Loss: 0.0198, Val Loss: 0.0088\n",
      "Epoch 13, Train Loss: 0.0201, Val Loss: 0.0054\n",
      "Epoch 14, Train Loss: 0.0167, Val Loss: 0.0070\n",
      "Epoch 15, Train Loss: 0.0154, Val Loss: 0.0065\n",
      "Epoch 16, Train Loss: 0.0082, Val Loss: 0.0043\n",
      "Epoch 17, Train Loss: 0.0147, Val Loss: 0.0040\n",
      "Epoch 18, Train Loss: 0.0082, Val Loss: 0.0015\n",
      "Epoch 19, Train Loss: 0.0112, Val Loss: 0.0014\n",
      "Epoch 20, Train Loss: 0.0088, Val Loss: 0.0011\n",
      "Epoch 21, Train Loss: 0.0065, Val Loss: 0.0018\n",
      "Epoch 22, Train Loss: 0.0089, Val Loss: 0.0005\n",
      "Epoch 23, Train Loss: 0.0041, Val Loss: 0.0005\n",
      "Epoch 24, Train Loss: 0.0051, Val Loss: 0.0003\n",
      "Epoch 25, Train Loss: 0.0033, Val Loss: 0.0005\n",
      "Epoch 26, Train Loss: 0.0047, Val Loss: 0.0002\n",
      "Epoch 27, Train Loss: 0.0069, Val Loss: 0.0021\n",
      "Epoch 28, Train Loss: 0.0047, Val Loss: 0.0002\n",
      "Epoch 29, Train Loss: 0.0029, Val Loss: 0.0011\n",
      "Epoch 30, Train Loss: 0.0068, Val Loss: 0.0013\n",
      "Epoch 31, Train Loss: 0.0025, Val Loss: 0.0009\n",
      "Epoch 32, Train Loss: 0.0052, Val Loss: 0.0006\n",
      "Epoch 33, Train Loss: 0.0062, Val Loss: 0.0001\n",
      "Epoch 34, Train Loss: 0.0019, Val Loss: 0.0001\n",
      "Epoch 35, Train Loss: 0.0015, Val Loss: 0.0001\n",
      "Epoch 36, Train Loss: 0.0021, Val Loss: 0.0001\n",
      "Epoch 37, Train Loss: 0.0022, Val Loss: 0.0001\n",
      "Epoch 38, Train Loss: 0.0018, Val Loss: 0.0001\n",
      "Epoch 39, Train Loss: 0.0017, Val Loss: 0.0001\n",
      "Epoch 40, Train Loss: 0.0020, Val Loss: 0.0001\n",
      "Epoch 41, Train Loss: 0.0029, Val Loss: 0.0001\n",
      "Epoch 42, Train Loss: 0.0018, Val Loss: 0.0001\n",
      "Epoch 43, Train Loss: 0.0022, Val Loss: 0.0001\n",
      "Epoch 44, Train Loss: 0.0019, Val Loss: 0.0001\n",
      "Epoch 45, Train Loss: 0.0017, Val Loss: 0.0001\n",
      "Epoch 46, Train Loss: 0.0022, Val Loss: 0.0001\n",
      "Epoch 47, Train Loss: 0.0015, Val Loss: 0.0001\n",
      "Early stopping triggered after 47 epochs\n",
      "Done training, time: 48.33 秒\n",
      "在验证集上寻找最佳阈值...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2257642/1044010578.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_gru_lstm_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在测试集上进行最终评估...\n",
      "Loading data...\n",
      "Preprocessing data...\n",
      "Training model...\n",
      "Epoch 1, Train Loss: 0.6269, Val Loss: 0.4630\n",
      "Epoch 2, Train Loss: 0.2789, Val Loss: 0.1628\n",
      "Epoch 3, Train Loss: 0.1183, Val Loss: 0.0735\n",
      "Epoch 4, Train Loss: 0.0599, Val Loss: 0.0276\n",
      "Epoch 5, Train Loss: 0.0487, Val Loss: 0.0234\n",
      "Epoch 6, Train Loss: 0.0322, Val Loss: 0.0148\n",
      "Epoch 7, Train Loss: 0.0233, Val Loss: 0.0141\n",
      "Epoch 8, Train Loss: 0.0218, Val Loss: 0.0062\n",
      "Epoch 9, Train Loss: 0.0235, Val Loss: 0.0054\n",
      "Epoch 10, Train Loss: 0.0202, Val Loss: 0.0052\n",
      "Epoch 11, Train Loss: 0.0148, Val Loss: 0.0025\n",
      "Epoch 12, Train Loss: 0.0073, Val Loss: 0.0023\n",
      "Epoch 13, Train Loss: 0.0110, Val Loss: 0.0176\n",
      "Epoch 14, Train Loss: 0.0163, Val Loss: 0.0030\n",
      "Epoch 15, Train Loss: 0.0072, Val Loss: 0.0014\n",
      "Epoch 16, Train Loss: 0.0054, Val Loss: 0.0012\n",
      "Epoch 17, Train Loss: 0.0068, Val Loss: 0.0013\n",
      "Epoch 18, Train Loss: 0.0073, Val Loss: 0.0030\n",
      "Epoch 19, Train Loss: 0.0034, Val Loss: 0.0010\n",
      "Epoch 20, Train Loss: 0.0043, Val Loss: 0.0009\n",
      "Epoch 21, Train Loss: 0.0051, Val Loss: 0.0053\n",
      "Epoch 22, Train Loss: 0.0081, Val Loss: 0.0030\n",
      "Epoch 23, Train Loss: 0.0034, Val Loss: 0.0023\n",
      "Epoch 24, Train Loss: 0.0047, Val Loss: 0.0044\n",
      "Epoch 25, Train Loss: 0.0074, Val Loss: 0.0011\n",
      "Epoch 26, Train Loss: 0.0035, Val Loss: 0.0014\n",
      "Epoch 27, Train Loss: 0.0032, Val Loss: 0.0011\n",
      "Epoch 28, Train Loss: 0.0022, Val Loss: 0.0011\n",
      "Epoch 29, Train Loss: 0.0023, Val Loss: 0.0011\n",
      "Epoch 30, Train Loss: 0.0031, Val Loss: 0.0012\n",
      "Early stopping triggered after 30 epochs\n",
      "Done training, time: 34.71 秒\n",
      "在验证集上寻找最佳阈值...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2257642/1044010578.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_gru_lstm_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在测试集上进行最终评估...\n",
      "Loading data...\n",
      "Preprocessing data...\n",
      "Training model...\n",
      "Epoch 1, Train Loss: 0.5864, Val Loss: 0.3409\n",
      "Epoch 2, Train Loss: 0.1442, Val Loss: 0.0842\n",
      "Epoch 3, Train Loss: 0.0758, Val Loss: 0.0459\n",
      "Epoch 4, Train Loss: 0.0430, Val Loss: 0.0193\n",
      "Epoch 5, Train Loss: 0.0255, Val Loss: 0.0113\n",
      "Epoch 6, Train Loss: 0.0149, Val Loss: 0.0090\n",
      "Epoch 7, Train Loss: 0.0132, Val Loss: 0.0063\n",
      "Epoch 8, Train Loss: 0.0156, Val Loss: 0.0091\n",
      "Epoch 9, Train Loss: 0.0119, Val Loss: 0.0034\n",
      "Epoch 10, Train Loss: 0.0073, Val Loss: 0.0023\n",
      "Epoch 11, Train Loss: 0.0066, Val Loss: 0.0043\n",
      "Epoch 12, Train Loss: 0.0102, Val Loss: 0.0015\n",
      "Epoch 13, Train Loss: 0.0035, Val Loss: 0.0013\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 368\u001b[0m\n\u001b[1;32m    363\u001b[0m             f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfusion Matrix: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_conf_matrix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 368\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[5], line 325\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    324\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 325\u001b[0m model, train_losses, val_losses\u001b[38;5;241m=\u001b[39m train_model(\n\u001b[1;32m    326\u001b[0m     model, \n\u001b[1;32m    327\u001b[0m     train_loader, \n\u001b[1;32m    328\u001b[0m     val_loader, \n\u001b[1;32m    329\u001b[0m     criterion, \n\u001b[1;32m    330\u001b[0m     optimizer,\n\u001b[1;32m    331\u001b[0m     scheduler,\n\u001b[1;32m    332\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, \n\u001b[1;32m    333\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    334\u001b[0m )\n\u001b[1;32m    335\u001b[0m training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone training, time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 秒\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 115\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience)\u001b[0m\n\u001b[1;32m    113\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    114\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m--> 115\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    116\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    118\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/optim/adamw.py:220\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    207\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    209\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    210\u001b[0m         group,\n\u001b[1;32m    211\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m         state_steps,\n\u001b[1;32m    218\u001b[0m     )\n\u001b[0;32m--> 220\u001b[0m     adamw(\n\u001b[1;32m    221\u001b[0m         params_with_grad,\n\u001b[1;32m    222\u001b[0m         grads,\n\u001b[1;32m    223\u001b[0m         exp_avgs,\n\u001b[1;32m    224\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    225\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    226\u001b[0m         state_steps,\n\u001b[1;32m    227\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    228\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    229\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    230\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    231\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    232\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    233\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    234\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    235\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    236\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    237\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    238\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    239\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    240\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    241\u001b[0m     )\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/optim/adamw.py:782\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    780\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 782\u001b[0m func(\n\u001b[1;32m    783\u001b[0m     params,\n\u001b[1;32m    784\u001b[0m     grads,\n\u001b[1;32m    785\u001b[0m     exp_avgs,\n\u001b[1;32m    786\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    787\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    788\u001b[0m     state_steps,\n\u001b[1;32m    789\u001b[0m     amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    790\u001b[0m     beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    791\u001b[0m     beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    792\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    793\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    794\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    795\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    796\u001b[0m     capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    797\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    798\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    799\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39mfound_inf,\n\u001b[1;32m    800\u001b[0m     has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    801\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/optim/adamw.py:427\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    425\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    429\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# 设置随机种子以确保结果可复现\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 检查是否有可用的GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 自定义数据集类\n",
    "class IntrusionDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# 定义GRU+LSTM\n",
    "class GRU_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size_gru=128, hidden_size_lstm=64, dropout=0.3):\n",
    "        super(GRU_LSTM, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size_gru, batch_first=True, bidirectional=True)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(hidden_size_gru*2, hidden_size_lstm, batch_first=True, bidirectional=True)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size_lstm*2, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        gru_out = self.dropout1(gru_out)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(gru_out)\n",
    "        lstm_out = self.dropout2(lstm_out[:, -1, :])\n",
    "        \n",
    "        out = self.relu(self.fc1(lstm_out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# 数据加载\n",
    "def load_data(train_self_path, train_nonself_path, test_self_path, test_nonself_path,unknown_path):\n",
    "    train_nonself = pd.read_csv(train_nonself_path)\n",
    "    train_self = pd.read_csv(train_self_path)\n",
    "    train_self = train_self.sample(n=len(train_nonself),random_state=42)\n",
    "\n",
    "    unknown = pd.read_csv(unknown_path)\n",
    "    \n",
    "    # 加载测试数据\n",
    "    test_self = pd.read_csv(test_self_path)\n",
    "    test_self = test_self.sample(n=5000,random_state=42)\n",
    "    test_nonself = pd.read_csv(test_nonself_path)\n",
    "    test_nonself = test_nonself.sample(n=5000,random_state=42)\n",
    "\n",
    "    # 添加标签：自体为0，非自体为1\n",
    "    train_self['label'] = 0\n",
    "    train_nonself['label'] = 1\n",
    "    test_self['label'] = 0\n",
    "    test_nonself['label'] = 1\n",
    "    \n",
    "    # 合并训练集和测试集\n",
    "    train_data = pd.concat([train_self, train_nonself], axis=0).reset_index(drop=True)\n",
    "    train_data = train_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    test_data = pd.concat([test_self, test_nonself], axis=0).reset_index(drop=True)\n",
    "    test_data = test_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return train_data, test_data, unknown\n",
    "\n",
    "# 数据预处理函数\n",
    "def preprocess_data(train_data, test_data):\n",
    "    # 处理缺失值\n",
    "    train_data = train_data.dropna()\n",
    "    test_data = test_data.dropna()\n",
    "    # 分离特征和标签\n",
    "    X_train = train_data.drop('label', axis=1).values\n",
    "    y_train = train_data['label'].values\n",
    "    X_test = test_data.drop('label', axis=1).values\n",
    "    y_test = test_data['label'].values\n",
    "    \n",
    "    X_train_seq = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "    X_test_seq = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "    \n",
    "    return X_train_seq, y_train, X_test_seq, y_test\n",
    "\n",
    "# 训练模型函数\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=50, patience=10):\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for features, labels in train_loader:\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_outputs = []\n",
    "        val_labels = []\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "                val_loss += loss.item()\n",
    "                val_outputs.extend(outputs.sigmoid().numpy())\n",
    "                val_labels.extend(labels.numpy())\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_gru_lstm_model.pth')\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(torch.load('best_gru_lstm_model.pth'))\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# 评估未知覆盖率和误报率\n",
    "def evaluate_unknown_coverage(model, unknown_data, threshold=0):\n",
    "    # 预处理未知数据\n",
    "    X_unknown = unknown_data.values\n",
    "    X_unknown_seq = X_unknown.reshape(X_unknown.shape[0], 1, X_unknown.shape[1])\n",
    "    unknown_dataset = torch.FloatTensor(X_unknown_seq)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    unknown_loader = DataLoader(unknown_dataset, batch_size=64)\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features in unknown_loader:\n",
    "            features = features.to(device)\n",
    "            outputs = model(features)\n",
    "            predicted = (outputs >= threshold).float()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds).flatten()\n",
    "    \n",
    "    # 计算未知覆盖率 - 被检测为异常的未知样本比例\n",
    "    unknown_coverage = np.mean(all_preds)\n",
    "    \n",
    "    return unknown_coverage\n",
    "\n",
    "# 评估误报率 - 在正常数据上\n",
    "def evaluate_false_positive_rate(model, normal_data, threshold=0):\n",
    "    # 预处理正常数据\n",
    "    X_normal = normal_data.drop('label', axis=1).values\n",
    "    X_normal_seq = X_normal.reshape(X_normal.shape[0], 1, X_normal.shape[1])\n",
    "    \n",
    "    normal_dataset = IntrusionDataset(X_normal_seq, np.zeros(len(X_normal)))\n",
    "    normal_loader = DataLoader(normal_dataset, batch_size=64)\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, _ in normal_loader:\n",
    "            features = features.to(device)\n",
    "            outputs = model(features)\n",
    "            predicted = (outputs >= threshold).float()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds).flatten()\n",
    "    \n",
    "    # 计算误报率 - 正常样本被错误分类为异常的比例\n",
    "    false_positive_rate = np.mean(all_preds)\n",
    "    \n",
    "    return false_positive_rate\n",
    "\n",
    "# 评估模型性能\n",
    "def evaluate_model(model, features, labels, threshold=0):\n",
    "    model.eval()\n",
    "    dataset = IntrusionDataset(features, labels)\n",
    "    test_loader = DataLoader(dataset, batch_size=64)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_labels in test_loader:\n",
    "            batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
    "            \n",
    "            outputs = model(batch_features)\n",
    "            predicted = (outputs >= threshold).float()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds).flatten()\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # 计算评估指标\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, conf_matrix\n",
    "\n",
    "# 计算最佳阈值\n",
    "def find_optimal_threshold(model, X_val_seq, y_val):\n",
    "    val_dataset = IntrusionDataset(X_val_seq, y_val)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "    \n",
    "    # 收集所有预测分数\n",
    "    model.eval()\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            all_scores.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_scores = np.array(all_scores).flatten()\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # 尝试不同阈值\n",
    "    thresholds = np.linspace(0, 1, 1000)\n",
    "    results = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        # 计算验证集上的F1分数\n",
    "        predicted = (all_scores >= threshold).astype(float)\n",
    "        f1 = f1_score(all_labels, predicted)\n",
    "\n",
    "        # 计算综合得分 (可以根据需要调整权重)\n",
    "        score = f1\n",
    "        results.append((threshold,f1,score))\n",
    "    \n",
    "    # 找到最佳阈值\n",
    "    best_result = max(results, key=lambda x: x[1])\n",
    "    return best_result\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    unknown_types = [\"bot\", \"bruteforce\", \"dos\", \"ddos\", \"infiltration\", \"sql_injection\"]\n",
    "    for unknown_type in unknown_types:\n",
    "    \n",
    "        # 设置数据路径\n",
    "        train_self_path = '../../check/self/train_self.csv'\n",
    "        train_nonself_path = f'../../check/train/seed_{unknown_type}.csv'\n",
    "        test_self_path = '../../check/self/test_self.csv'\n",
    "        test_nonself_path = '../../check/nonself/test_nonself.csv'\n",
    "        unknown_path = f'../../check/unknown/{unknown_type}.csv'\n",
    "        \n",
    "        # 加载数据\n",
    "        print(\"Loading data...\")\n",
    "        train_data, test_data, unknown = load_data(train_self_path, train_nonself_path, test_self_path, test_nonself_path,unknown_path)\n",
    "        \n",
    "        # 预处理数据\n",
    "        print(\"Preprocessing data...\")\n",
    "        X_train, y_train, X_test, y_test = preprocess_data(train_data, test_data)\n",
    "        \n",
    "        # 将训练集分为训练集和验证集（80%训练，20%验证）\n",
    "        train_size = int(0.8 * len(X_train))\n",
    "        val_size = len(X_train) - train_size\n",
    "        \n",
    "        X_train_split, X_val = X_train[:train_size], X_train[train_size:]\n",
    "        y_train_split, y_val = y_train[:train_size], y_train[train_size:]\n",
    "        \n",
    "        train_dataset = IntrusionDataset(X_train_split, y_train_split)\n",
    "        val_dataset = IntrusionDataset(X_val, y_val)\n",
    "        test_dataset = IntrusionDataset(X_test, y_test)\n",
    "        batch_size = 64\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "        \n",
    "        input_size = X_train.shape[2]\n",
    "        model = GRU_LSTM(input_size)\n",
    "        \n",
    "        # 权重\n",
    "        pos_weight = torch.tensor([(y_train == 0).sum() / [(y_train == 1).sum()] ])\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
    "        \n",
    "        # 使用带权重衰减的优化器\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "        \n",
    "        # 添加学习率调度器\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.1)\n",
    "        \n",
    "        # 训练模型\n",
    "        print(\"Training model...\")\n",
    "        start_time = time.time()\n",
    "        model, train_losses, val_losses= train_model(\n",
    "            model, \n",
    "            train_loader, \n",
    "            val_loader, \n",
    "            criterion, \n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            num_epochs=50, \n",
    "            patience=10\n",
    "        )\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"Done training, time: {training_time:.2f} 秒\")\n",
    "\n",
    "        # 使用验证集找到最佳阈值\n",
    "        print(\"在验证集上寻找最佳阈值...\")\n",
    "        best_result = find_optimal_threshold(model, X_val, y_val)\n",
    "        best_threshold = best_result[0]\n",
    "        \n",
    "        # 使用测试集进行最终评估\n",
    "        print(\"在测试集上进行最终评估...\")\n",
    "        test_accuracy, test_precision, test_recall, test_f1, test_conf_matrix = evaluate_model(\n",
    "            model, X_test, y_test, threshold=best_threshold\n",
    "        )\n",
    "\n",
    "        # 在测试集上评估未知覆盖率和误报率\n",
    "        test_unknown_coverage = evaluate_unknown_coverage(model, unknown, threshold=best_threshold)\n",
    "        test_self_data = test_data[test_data['label'] == 0]\n",
    "        test_fpr = evaluate_false_positive_rate(model, test_self_data, threshold=best_threshold)\n",
    "        \n",
    "        with open(f'{unknown_type}_results.txt', 'w') as f:\n",
    "            f.write(f\"Best Threshold: {best_threshold:.6f}\\n\")\n",
    "            f.write(\"\\nTest Set Results:\\n\")\n",
    "            f.write(f\"Accuracy: {test_accuracy:.4f}\\n\")\n",
    "            f.write(f\"Precision: {test_precision:.4f}\\n\")\n",
    "            f.write(f\"Recall: {test_recall:.4f}\\n\")\n",
    "            f.write(f\"F1 Score: {test_f1:.4f}\\n\")\n",
    "            f.write(f\"Unknown Coverage: {test_unknown_coverage:.4f}\\n\")\n",
    "            f.write(f\"False Positive Rate: {test_fpr:.4f}\\n\")\n",
    "            f.write(f\"Confusion Matrix: {test_conf_matrix}\\n\")\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
