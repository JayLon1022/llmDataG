{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cpu\n",
      "Loading data...\n",
      "训练集分布：\n",
      "label\n",
      "0    1500\n",
      "1     720\n",
      "Name: count, dtype: int64\n",
      "\n",
      "测试集分布：\n",
      "label\n",
      "0    5000\n",
      "1    5000\n",
      "Name: count, dtype: int64\n",
      "Preprocessing data...\n",
      "Training model...\n",
      "Epoch 1, Train Loss: 0.7096, Val Loss: 2.0834\n",
      "Epoch 2, Train Loss: 0.5851, Val Loss: 1.8998\n",
      "Epoch 3, Train Loss: 0.4570, Val Loss: 0.8809\n",
      "Epoch 4, Train Loss: 0.3778, Val Loss: 1.0102\n",
      "Epoch 5, Train Loss: 0.3540, Val Loss: 0.9507\n",
      "Epoch 6, Train Loss: 0.3442, Val Loss: 0.8655\n",
      "Epoch 7, Train Loss: 0.3196, Val Loss: 0.7082\n",
      "Epoch 8, Train Loss: 0.2521, Val Loss: 1.0821\n",
      "Epoch 9, Train Loss: 0.2745, Val Loss: 0.8212\n",
      "Epoch 10, Train Loss: 0.2245, Val Loss: 0.7220\n",
      "Epoch 11, Train Loss: 0.2211, Val Loss: 0.4863\n",
      "Epoch 12, Train Loss: 0.2132, Val Loss: 0.6406\n",
      "Epoch 13, Train Loss: 0.2282, Val Loss: 0.6267\n",
      "Epoch 14, Train Loss: 0.1858, Val Loss: 0.5353\n",
      "Epoch 15, Train Loss: 0.2027, Val Loss: 0.3811\n",
      "Epoch 16, Train Loss: 0.1937, Val Loss: 0.2621\n",
      "Epoch 17, Train Loss: 0.1823, Val Loss: 0.4370\n",
      "Epoch 18, Train Loss: 0.1717, Val Loss: 0.3713\n",
      "Epoch 19, Train Loss: 0.1838, Val Loss: 0.3445\n",
      "Epoch 20, Train Loss: 0.1574, Val Loss: 0.8996\n",
      "Epoch 21, Train Loss: 0.1668, Val Loss: 0.3213\n",
      "Early stopping triggered after 21 epochs\n",
      "Done training, time: 27.58 秒\n",
      "Evaluating model...\n",
      "Results saved to dos/cnn_lstm_results.txt\n",
      "Loading data...\n",
      "训练集分布：\n",
      "label\n",
      "0    1500\n",
      "1    1220\n",
      "Name: count, dtype: int64\n",
      "\n",
      "测试集分布：\n",
      "label\n",
      "0    5000\n",
      "1    5000\n",
      "Name: count, dtype: int64\n",
      "Preprocessing data...\n",
      "Training model...\n",
      "Epoch 1, Train Loss: 0.5298, Val Loss: 0.4832\n",
      "Epoch 2, Train Loss: 0.2948, Val Loss: 0.3796\n",
      "Epoch 3, Train Loss: 0.1588, Val Loss: 0.1609\n",
      "Epoch 4, Train Loss: 0.1614, Val Loss: 0.1289\n",
      "Epoch 5, Train Loss: 0.1173, Val Loss: 0.0844\n",
      "Epoch 6, Train Loss: 0.0876, Val Loss: 0.0726\n",
      "Epoch 7, Train Loss: 0.0702, Val Loss: 0.1672\n",
      "Epoch 8, Train Loss: 0.0961, Val Loss: 0.2364\n",
      "Epoch 9, Train Loss: 0.0833, Val Loss: 0.2974\n",
      "Epoch 10, Train Loss: 0.0734, Val Loss: 0.0896\n",
      "Epoch 11, Train Loss: 0.0410, Val Loss: 0.1398\n",
      "Early stopping triggered after 11 epochs\n",
      "Done training, time: 116.22 秒\n",
      "Evaluating model...\n",
      "Results saved to probe/cnn_lstm_results.txt\n",
      "Loading data...\n",
      "训练集分布：\n",
      "label\n",
      "1    1511\n",
      "0    1500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "测试集分布：\n",
      "label\n",
      "0    5000\n",
      "1    5000\n",
      "Name: count, dtype: int64\n",
      "Preprocessing data...\n",
      "Training model...\n",
      "Epoch 1, Train Loss: 0.5358, Val Loss: 0.4878\n",
      "Epoch 2, Train Loss: 0.2614, Val Loss: 0.3097\n",
      "Epoch 3, Train Loss: 0.2562, Val Loss: 0.3650\n",
      "Epoch 4, Train Loss: 0.2102, Val Loss: 0.3741\n",
      "Epoch 5, Train Loss: 0.2532, Val Loss: 0.2558\n",
      "Epoch 6, Train Loss: 0.1732, Val Loss: 0.2838\n",
      "Epoch 7, Train Loss: 0.1688, Val Loss: 0.2393\n",
      "Epoch 8, Train Loss: 0.1658, Val Loss: 0.1870\n",
      "Epoch 9, Train Loss: 0.1258, Val Loss: 0.1895\n",
      "Epoch 10, Train Loss: 0.1179, Val Loss: 0.1409\n",
      "Epoch 11, Train Loss: 0.1314, Val Loss: 0.3006\n",
      "Epoch 12, Train Loss: 0.1174, Val Loss: 0.1773\n",
      "Epoch 13, Train Loss: 0.1413, Val Loss: 0.1023\n",
      "Epoch 14, Train Loss: 0.1220, Val Loss: 0.2240\n",
      "Epoch 15, Train Loss: 0.1052, Val Loss: 0.0984\n",
      "Epoch 16, Train Loss: 0.1120, Val Loss: 0.1526\n",
      "Epoch 17, Train Loss: 0.1085, Val Loss: 0.0866\n",
      "Epoch 18, Train Loss: 0.0959, Val Loss: 0.2225\n",
      "Epoch 19, Train Loss: 0.0929, Val Loss: 0.2577\n",
      "Epoch 20, Train Loss: 0.0921, Val Loss: 0.2564\n",
      "Epoch 21, Train Loss: 0.1051, Val Loss: 0.1122\n",
      "Epoch 22, Train Loss: 0.0779, Val Loss: 0.1630\n",
      "Early stopping triggered after 22 epochs\n",
      "Done training, time: 45.62 秒\n",
      "Evaluating model...\n",
      "Results saved to r2l/cnn_lstm_results.txt\n",
      "Loading data...\n",
      "训练集分布：\n",
      "label\n",
      "1    1709\n",
      "0    1500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "测试集分布：\n",
      "label\n",
      "0    5000\n",
      "1    5000\n",
      "Name: count, dtype: int64\n",
      "Preprocessing data...\n",
      "Training model...\n",
      "Epoch 1, Train Loss: 0.5019, Val Loss: 0.3876\n",
      "Epoch 2, Train Loss: 0.2759, Val Loss: 0.3717\n",
      "Epoch 3, Train Loss: 0.2278, Val Loss: 0.1271\n",
      "Epoch 4, Train Loss: 0.1782, Val Loss: 0.5094\n",
      "Epoch 5, Train Loss: 0.1611, Val Loss: 0.1575\n",
      "Epoch 6, Train Loss: 0.1651, Val Loss: 0.1301\n",
      "Epoch 7, Train Loss: 0.1385, Val Loss: 0.0326\n",
      "Epoch 8, Train Loss: 0.1206, Val Loss: 0.2141\n",
      "Epoch 9, Train Loss: 0.1228, Val Loss: 0.1345\n",
      "Epoch 10, Train Loss: 0.1336, Val Loss: 0.0502\n",
      "Epoch 11, Train Loss: 0.1059, Val Loss: 0.2115\n",
      "Epoch 12, Train Loss: 0.0861, Val Loss: 0.0746\n",
      "Early stopping triggered after 12 epochs\n",
      "Done training, time: 24.97 秒\n",
      "Evaluating model...\n",
      "Results saved to u2r/cnn_lstm_results.txt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "\n",
    "# 设置随机种子以确保结果可复现\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 检查是否有可用的GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 自定义数据集类\n",
    "class IntrusionDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# 定义CNN+LSTM模型\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, cnn_out_channels=64, lstm_hidden_size=128, dropout=0.3):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        \n",
    "        # CNN层\n",
    "        self.conv1 = nn.Conv1d(1, cnn_out_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=1)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # LSTM层\n",
    "        self.lstm = nn.LSTM(cnn_out_channels, lstm_hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # 全连接层\n",
    "        self.fc1 = nn.Linear(lstm_hidden_size*2, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 输入x的形状: [batch_size, seq_len, features]\n",
    "        # 转换为CNN的输入形状: [batch_size, channels, features]\n",
    "        x = x.permute(0, 1, 2)\n",
    "        \n",
    "        # CNN处理\n",
    "        cnn_out = self.conv1(x)\n",
    "        cnn_out = self.relu(cnn_out)\n",
    "        cnn_out = self.maxpool(cnn_out)\n",
    "        cnn_out = self.dropout1(cnn_out)\n",
    "        \n",
    "        # 转换为LSTM的输入形状: [batch_size, seq_len, features]\n",
    "        cnn_out = cnn_out.permute(0, 2, 1)\n",
    "        \n",
    "        # LSTM处理\n",
    "        lstm_out, _ = self.lstm(cnn_out)\n",
    "        lstm_out = self.dropout2(lstm_out[:, -1, :])  # 取最后一个时间步的输出\n",
    "        \n",
    "        # 全连接层\n",
    "        out = self.relu(self.fc1(lstm_out))\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# 数据加载\n",
    "def load_data(train_self_path, train_nonself_path, test_self_path, test_nonself_path, unknown_path):\n",
    "    \n",
    "    train_self = pd.read_csv(train_self_path)\n",
    "    train_self = train_self.sample(n=1500, random_state=42)\n",
    "    train_nonself = pd.read_csv(train_nonself_path)\n",
    "    \n",
    "    unknown = pd.read_csv(unknown_path)\n",
    "    \n",
    "    # 加载测试数据\n",
    "    test_self = pd.read_csv(test_self_path)\n",
    "    test_self = test_self.sample(n=5000, random_state=42)\n",
    "    test_nonself = pd.read_csv(test_nonself_path)\n",
    "    test_nonself = test_nonself.sample(n=5000, random_state=42)\n",
    "    \n",
    "    # 添加标签：自体为0，非自体为1\n",
    "    train_self['label'] = 0\n",
    "    train_nonself['label'] = 1\n",
    "    test_self['label'] = 0\n",
    "    test_nonself['label'] = 1\n",
    "    \n",
    "    # 合并训练集和测试集\n",
    "    train_data = pd.concat([train_self, train_nonself], axis=0).reset_index(drop=True)\n",
    "    test_data = pd.concat([test_self, test_nonself], axis=0).reset_index(drop=True)\n",
    "    print(\"训练集分布：\")\n",
    "    print(train_data['label'].value_counts())\n",
    "    print(\"\\n测试集分布：\")\n",
    "    print(test_data['label'].value_counts())\n",
    "    return train_data, test_data, unknown\n",
    "\n",
    "# 数据预处理函数\n",
    "def preprocess_data(train_data, test_data):\n",
    "    # 处理缺失值\n",
    "    train_data = train_data.fillna(train_data.mean())\n",
    "    test_data = test_data.fillna(test_data.mean())\n",
    "    # 分离特征和标签\n",
    "    X_train = train_data.drop('label', axis=1).values\n",
    "    y_train = train_data['label'].values\n",
    "    X_test = test_data.drop('label', axis=1).values\n",
    "    y_test = test_data['label'].values\n",
    "    \n",
    "    X_train_seq = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "    X_test_seq = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "    \n",
    "    return X_train_seq, y_train, X_test_seq, y_test\n",
    "\n",
    "# 训练模型函数\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=20, patience=5):\n",
    "    model.to(device)\n",
    "    \n",
    "    # 用于早停的变量\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    # 记录训练和验证损失\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # 训练阶段\n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # 早停检查\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_cnn_lstm_model.pth')\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "                break\n",
    "    \n",
    "    # 加载最佳模型\n",
    "    model.load_state_dict(torch.load('best_cnn_lstm_model.pth', weights_only=True))\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# 评估未知覆盖率和误报率\n",
    "def evaluate_unknown_coverage(model, unknown_data, threshold=0):\n",
    "    # 预处理未知数据\n",
    "    X_unknown = unknown_data.values\n",
    "    X_unknown_seq = X_unknown.reshape(X_unknown.shape[0], 1, X_unknown.shape[1])\n",
    "    unknown_dataset = torch.FloatTensor(X_unknown_seq)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    unknown_loader = DataLoader(unknown_dataset, batch_size=64)\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features in unknown_loader:\n",
    "            features = features.to(device)\n",
    "            outputs = model(features)\n",
    "            predicted = (outputs >= threshold).float()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds).flatten()\n",
    "    \n",
    "    # 计算未知覆盖率 - 被检测为异常的未知样本比例\n",
    "    unknown_coverage = np.mean(all_preds)\n",
    "    \n",
    "    return unknown_coverage\n",
    "\n",
    "# 评估误报率 - 在正常数据上\n",
    "def evaluate_false_positive_rate(model, normal_data, threshold=0):\n",
    "    # 预处理正常数据\n",
    "    X_normal = normal_data.drop('label', axis=1).values\n",
    "    X_normal_seq = X_normal.reshape(X_normal.shape[0], 1, X_normal.shape[1])\n",
    "    \n",
    "    normal_dataset = IntrusionDataset(X_normal_seq, np.zeros(len(X_normal)))\n",
    "    normal_loader = DataLoader(normal_dataset, batch_size=64)\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, _ in normal_loader:\n",
    "            features = features.to(device)\n",
    "            outputs = model(features)\n",
    "            predicted = (outputs >= threshold).float()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds).flatten()\n",
    "    \n",
    "    # 计算误报率 - 正常样本被错误分类为异常的比例\n",
    "    false_positive_rate = np.mean(all_preds)\n",
    "    \n",
    "    return false_positive_rate\n",
    "\n",
    "# 评估模型性能\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(features)\n",
    "            predicted = (outputs >= 0).float()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds).flatten()\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # 计算评估指标\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, conf_matrix\n",
    "\n",
    "# 计算最佳阈值\n",
    "def find_optimal_threshold(model, X_val_seq, y_val, unknown_data, normal_data):\n",
    "    val_dataset = IntrusionDataset(X_val_seq, y_val)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "    \n",
    "    # 收集所有预测分数\n",
    "    model.eval()\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            all_scores.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_scores = np.array(all_scores).flatten()\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # 尝试不同阈值\n",
    "    thresholds = np.linspace(np.min(all_scores), np.max(all_scores), 100)\n",
    "    results = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        # 计算验证集上的F1分数\n",
    "        predicted = (all_scores >= threshold).astype(float)\n",
    "        f1 = f1_score(all_labels, predicted)\n",
    "        \n",
    "        # 计算未知覆盖率\n",
    "        unknown_cov = evaluate_unknown_coverage(model, unknown_data, threshold)\n",
    "        \n",
    "        # 计算误报率\n",
    "        fpr = evaluate_false_positive_rate(model, normal_data, threshold)\n",
    "        \n",
    "        # 计算综合得分 (可以根据需要调整权重)\n",
    "        score = f1 * 0.4 + unknown_cov * 0.4 - fpr * 0.2\n",
    "        \n",
    "        results.append((threshold, f1, unknown_cov, fpr, score))\n",
    "    \n",
    "    # 找到最佳阈值\n",
    "    best_result = max(results, key=lambda x: x[4])\n",
    "    return best_result\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    unknown_types = [\"dos\", \"probe\", \"r2l\", \"u2r\"]\n",
    "    for unknown_type in unknown_types:\n",
    "        # 创建保存结果的目录\n",
    "        if not os.path.exists(unknown_type):\n",
    "            os.makedirs(unknown_type)\n",
    "            \n",
    "        # 设置数据路径\n",
    "        train_self_path = '../../check/self/train_self.csv'\n",
    "        train_nonself_path = f'../../check/train/seed_{unknown_type}.csv'\n",
    "        test_self_path = '../../check/self/test_self.csv'\n",
    "        test_nonself_path = '../../check/nonself/test_nonself.csv'\n",
    "        unknown_path = f'../../check/unknown/4type/{unknown_type}.csv'\n",
    "        \n",
    "        # 加载数据\n",
    "        print(\"Loading data...\")\n",
    "        train_data, test_data, unknown = load_data(train_self_path, train_nonself_path, test_self_path, test_nonself_path, unknown_path)\n",
    "        # 预处理数据\n",
    "        print(\"Preprocessing data...\")\n",
    "        X_train, y_train, X_test, y_test = preprocess_data(train_data, test_data)\n",
    "        \n",
    "        # 创建数据集和数据加载器\n",
    "        # 将训练集分为训练集和验证集（80%训练，20%验证）\n",
    "        train_size = int(0.8 * len(X_train))\n",
    "        val_size = len(X_train) - train_size\n",
    "        \n",
    "        X_train_split, X_val = X_train[:train_size], X_train[train_size:]\n",
    "        y_train_split, y_val = y_train[:train_size], y_train[train_size:]\n",
    "        \n",
    "        train_dataset = IntrusionDataset(X_train_split, y_train_split)\n",
    "        val_dataset = IntrusionDataset(X_val, y_val)\n",
    "        test_dataset = IntrusionDataset(X_test, y_test)\n",
    "        \n",
    "        batch_size = 64\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "        \n",
    "        # 创建模型\n",
    "        input_size = X_train.shape[2]  # 特征维度\n",
    "        model = CNN_LSTM(input_size)\n",
    "        \n",
    "        # 计算正样本权重\n",
    "        pos_weight = torch.tensor([(y_train == 0).sum() / (y_train == 1).sum()])\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
    "        \n",
    "        # 使用带权重衰减的优化器\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "        \n",
    "        # 添加学习率调度器\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.1)\n",
    "        \n",
    "        # 训练模型\n",
    "        print(\"Training model...\")\n",
    "        start_time = time.time()\n",
    "        model, train_losses, val_losses = train_model(\n",
    "            model, \n",
    "            train_loader, \n",
    "            val_loader, \n",
    "            criterion, \n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            num_epochs=30, \n",
    "            patience=5\n",
    "        )\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"Done training, time: {training_time:.2f} 秒\")\n",
    "        \n",
    "        # 评估模型\n",
    "        print(\"Evaluating model...\")\n",
    "        accuracy, precision, recall, f1, conf_matrix = evaluate_model(model, test_loader)\n",
    "        unknown_coverage = evaluate_unknown_coverage(model, unknown)\n",
    "        test_self_data = test_data[test_data['label'] == 0]\n",
    "        false_positive_rate = evaluate_false_positive_rate(model, test_self_data)\n",
    "        \n",
    "        # 保存结果\n",
    "        results = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'unknown_coverage': unknown_coverage,\n",
    "            'false_positive_rate': false_positive_rate,\n",
    "            'training_time': training_time,\n",
    "            'confusion_matrix': conf_matrix.tolist()\n",
    "        }\n",
    "        \n",
    "        # 将结果保存为文本文件\n",
    "        with open(f'{unknown_type}/cnn_lstm_results.txt', 'w') as f:\n",
    "            for key, value in results.items():\n",
    "                if key != 'confusion_matrix':\n",
    "                    f.write(f\"{key}: {value}\\n\")\n",
    "            f.write(f\"confusion_matrix:\\n{conf_matrix}\\n\")\n",
    "        \n",
    "        print(f\"Results saved to {unknown_type}/cnn_lstm_results.txt\")\n",
    "\n",
    "        # 计算最佳阈值\n",
    "        validation_data = pd.concat([train_data, test_data], axis=0).sample(frac=0.2, random_state=42)\n",
    "        X_val_data = validation_data.drop('label', axis=1).values\n",
    "        y_val_data = validation_data['label'].values\n",
    "        X_val_seq = X_val_data.reshape(X_val_data.shape[0], 1, X_val_data.shape[1])\n",
    "        best_result = find_optimal_threshold(model, X_val_seq, y_val_data, unknown, test_self_data)\n",
    "        best_threshold, best_f1, best_unknown_cov, best_fpr, best_score = best_result\n",
    "        with open(f'{unknown_type}/best_threshold_results.txt', 'w') as f:\n",
    "            f.write(f\"Best Threshold: {best_threshold:.6f}\\n\")\n",
    "            f.write(f\"F1 Score at Best Threshold: {best_f1:.4f}\\n\")\n",
    "            f.write(f\"Unknown Coverage at Best Threshold: {best_unknown_cov:.4f}\\n\")\n",
    "            f.write(f\"False Positive Rate at Best Threshold: {best_fpr:.4f}\\n\")\n",
    "            f.write(f\"Combined Score: {best_score:.4f}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义数据集类\n",
    "class IntrusionDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# 定义CNN+LSTM模型\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, cnn_out_channels=64, lstm_hidden_size=128, dropout=0.3):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        \n",
    "        # CNN层\n",
    "        self.conv1 = nn.Conv1d(1, cnn_out_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=1)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # LSTM层\n",
    "        self.lstm = nn.LSTM(cnn_out_channels, lstm_hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # 全连接层\n",
    "        self.fc1 = nn.Linear(lstm_hidden_size*2, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 输入x的形状: [batch_size, seq_len, features]\n",
    "        # 转换为CNN的输入形状: [batch_size, channels, features]\n",
    "        x = x.permute(0, 1, 2)\n",
    "        \n",
    "        # CNN处理\n",
    "        cnn_out = self.conv1(x)\n",
    "        cnn_out = self.relu(cnn_out)\n",
    "        cnn_out = self.maxpool(cnn_out)\n",
    "        cnn_out = self.dropout1(cnn_out)\n",
    "        \n",
    "        # 转换为LSTM的输入形状: [batch_size, seq_len, features]\n",
    "        cnn_out = cnn_out.permute(0, 2, 1)\n",
    "        \n",
    "        # LSTM处理\n",
    "        lstm_out, _ = self.lstm(cnn_out)\n",
    "        lstm_out = self.dropout2(lstm_out[:, -1, :])  # 取最后一个时间步的输出\n",
    "        \n",
    "        # 全连接层\n",
    "        out = self.relu(self.fc1(lstm_out))\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# 数据加载\n",
    "def load_data(train_self_path, train_nonself_path, test_self_path, test_nonself_path, test_unknown_path, train_unknown_path):\n",
    "    \n",
    "    train_self = pd.read_csv(train_self_path)\n",
    "    train_self = train_self.sample(n=1500, random_state=42)\n",
    "    train_nonself = pd.read_csv(train_nonself_path)\n",
    "    \n",
    "    test_unknown = pd.read_csv(test_unknown_path)\n",
    "    train_unknown = pd.read_csv(train_unknown_path)\n",
    "    unknown = pd.concat([test_unknown, train_unknown], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    # 加载测试数据\n",
    "    test_self = pd.read_csv(test_self_path)\n",
    "    test_self = test_self.sample(n=5000, random_state=42)\n",
    "    test_nonself = pd.read_csv(test_nonself_path)\n",
    "    test_nonself = test_nonself.sample(n=5000, random_state=42)\n",
    "    \n",
    "    # 添加标签：自体为0，非自体为1\n",
    "    train_self['label'] = 0\n",
    "    train_nonself['label'] = 1\n",
    "    test_self['label'] = 0\n",
    "    test_nonself['label'] = 1\n",
    "    \n",
    "    # 合并训练集和测试集\n",
    "    train_data = pd.concat([train_self, train_nonself], axis=0).reset_index(drop=True)\n",
    "    test_data = pd.concat([test_self, test_nonself], axis=0).reset_index(drop=True)\n",
    "    print(\"训练集分布：\")\n",
    "    print(train_data['label'].value_counts())\n",
    "    print(\"\\n测试集分布：\")\n",
    "    print(test_data['label'].value_counts())\n",
    "    return train_data, test_data, unknown\n",
    "\n",
    "# 数据预处理函数\n",
    "def preprocess_data(train_data, test_data):\n",
    "    # 分离特征和标签\n",
    "    X_train = train_data.drop('label', axis=1).values\n",
    "    y_train = train_data['label'].values\n",
    "    X_test = test_data.drop('label', axis=1).values\n",
    "    y_test = test_data['label'].values\n",
    "    \n",
    "    X_train_seq = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "    X_test_seq = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "    \n",
    "    return X_train_seq, y_train, X_test_seq, y_test\n",
    "\n",
    "# 训练模型函数\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=20, patience=5):\n",
    "    model.to(device)\n",
    "    \n",
    "    # 用于早停的变量\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    # 记录训练和验证损失\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # 训练阶段\n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # 早停检查\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_cnn_lstm_model.pth')\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "                break\n",
    "    \n",
    "    # 加载最佳模型\n",
    "    model.load_state_dict(torch.load('best_cnn_lstm_model.pth', weights_only=True))\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# 评估未知覆盖率和误报率\n",
    "def evaluate_unknown_coverage(model, unknown_data, threshold=0):\n",
    "    # 预处理未知数据\n",
    "    X_unknown = unknown_data.values\n",
    "    X_unknown_seq = X_unknown.reshape(X_unknown.shape[0], 1, X_unknown.shape[1])\n",
    "    unknown_dataset = torch.FloatTensor(X_unknown_seq)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    unknown_loader = DataLoader(unknown_dataset, batch_size=64)\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features in unknown_loader:\n",
    "            features = features.to(device)\n",
    "            outputs = model(features)\n",
    "            predicted = (outputs >= threshold).float()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds).flatten()\n",
    "    \n",
    "    # 计算未知覆盖率 - 被检测为异常的未知样本比例\n",
    "    unknown_coverage = np.mean(all_preds)\n",
    "    \n",
    "    return unknown_coverage\n",
    "\n",
    "# 评估误报率 - 在正常数据上\n",
    "def evaluate_false_positive_rate(model, normal_data, threshold=0):\n",
    "    # 预处理正常数据\n",
    "    X_normal = normal_data.drop('label', axis=1).values\n",
    "    X_normal_seq = X_normal.reshape(X_normal.shape[0], 1, X_normal.shape[1])\n",
    "    \n",
    "    normal_dataset = IntrusionDataset(X_normal_seq, np.zeros(len(X_normal)))\n",
    "    normal_loader = DataLoader(normal_dataset, batch_size=64)\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, _ in normal_loader:\n",
    "            features = features.to(device)\n",
    "            outputs = model(features)\n",
    "            predicted = (outputs >= threshold).float()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds).flatten()\n",
    "    \n",
    "    # 计算误报率 - 正常样本被错误分类为异常的比例\n",
    "    false_positive_rate = np.mean(all_preds)\n",
    "    \n",
    "    return false_positive_rate\n",
    "\n",
    "# 评估模型性能\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(features)\n",
    "            predicted = (outputs >= 0).float()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds).flatten()\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # 计算评估指标\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, conf_matrix\n",
    "\n",
    "# 计算最佳阈值\n",
    "def find_optimal_threshold(model, X_val_seq, y_val, unknown_data, normal_data):\n",
    "    val_dataset = IntrusionDataset(X_val_seq, y_val)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "    \n",
    "    # 收集所有预测分数\n",
    "    model.eval()\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            all_scores.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_scores = np.array(all_scores).flatten()\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # 尝试不同阈值\n",
    "    thresholds = np.linspace(np.min(all_scores), np.max(all_scores), 100)\n",
    "    results = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        # 计算验证集上的F1分数\n",
    "        predicted = (all_scores >= threshold).astype(float)\n",
    "        f1 = f1_score(all_labels, predicted)\n",
    "        \n",
    "        # 计算未知覆盖率\n",
    "        unknown_cov = evaluate_unknown_coverage(model, unknown_data, threshold)\n",
    "        \n",
    "        # 计算误报率\n",
    "        fpr = evaluate_false_positive_rate(model, normal_data, threshold)\n",
    "        \n",
    "        # 计算综合得分 (可以根据需要调整权重)\n",
    "        score = f1 * 0.4 + unknown_cov * 0.4 - fpr * 0.2\n",
    "        \n",
    "        results.append((threshold, f1, unknown_cov, fpr, score))\n",
    "    \n",
    "    # 找到最佳阈值\n",
    "    best_result = max(results, key=lambda x: x[4])\n",
    "    return best_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "训练集分布：\n",
      "label\n",
      "0    1500\n",
      "1    1040\n",
      "Name: count, dtype: int64\n",
      "\n",
      "测试集分布：\n",
      "label\n",
      "0    5000\n",
      "1    5000\n",
      "Name: count, dtype: int64\n",
      "Preprocessing data...\n",
      "Training model...\n",
      "Epoch 1, Train Loss: 0.7268, Val Loss: 1.3817\n",
      "Epoch 2, Train Loss: 0.7100, Val Loss: 1.5960\n",
      "Epoch 3, Train Loss: 0.7023, Val Loss: 1.4962\n",
      "Epoch 4, Train Loss: 0.6745, Val Loss: 1.5328\n",
      "Epoch 5, Train Loss: 0.5148, Val Loss: 1.5631\n",
      "Epoch 6, Train Loss: 0.4077, Val Loss: 1.3567\n",
      "Epoch 7, Train Loss: 0.3717, Val Loss: 1.4277\n",
      "Epoch 8, Train Loss: 0.3932, Val Loss: 1.4758\n",
      "Epoch 9, Train Loss: 0.3547, Val Loss: 1.4441\n",
      "Epoch 10, Train Loss: 0.3479, Val Loss: 1.4608\n",
      "Epoch 11, Train Loss: 0.3392, Val Loss: 1.4551\n",
      "Early stopping triggered after 11 epochs\n",
      "Done training, time: 52.67 秒\n",
      "Evaluating model...\n",
      "Results saved to A/cnn_lstm_results.txt\n",
      "Loading data...\n",
      "训练集分布：\n",
      "label\n",
      "0    1500\n",
      "1    1040\n",
      "Name: count, dtype: int64\n",
      "\n",
      "测试集分布：\n",
      "label\n",
      "0    5000\n",
      "1    5000\n",
      "Name: count, dtype: int64\n",
      "Preprocessing data...\n",
      "Training model...\n",
      "Epoch 1, Train Loss: 0.7210, Val Loss: 1.3721\n",
      "Epoch 2, Train Loss: 0.7070, Val Loss: 1.5780\n",
      "Epoch 3, Train Loss: 0.6556, Val Loss: 1.4728\n",
      "Epoch 4, Train Loss: 0.5523, Val Loss: 0.5923\n",
      "Epoch 5, Train Loss: 0.5862, Val Loss: 0.5846\n",
      "Epoch 6, Train Loss: 0.4359, Val Loss: 1.0932\n",
      "Epoch 7, Train Loss: 0.5317, Val Loss: 0.3473\n",
      "Epoch 8, Train Loss: 0.4382, Val Loss: 0.8333\n",
      "Epoch 9, Train Loss: 0.3916, Val Loss: 0.6205\n",
      "Epoch 10, Train Loss: 0.3707, Val Loss: 0.7107\n",
      "Epoch 11, Train Loss: 0.3409, Val Loss: 0.6560\n",
      "Epoch 12, Train Loss: 0.2844, Val Loss: 0.3749\n",
      "Early stopping triggered after 12 epochs\n",
      "Done training, time: 57.42 秒\n",
      "Evaluating model...\n",
      "Results saved to B/cnn_lstm_results.txt\n",
      "Loading data...\n",
      "训练集分布：\n",
      "label\n",
      "0    1500\n",
      "1    1040\n",
      "Name: count, dtype: int64\n",
      "\n",
      "测试集分布：\n",
      "label\n",
      "0    5000\n",
      "1    5000\n",
      "Name: count, dtype: int64\n",
      "Preprocessing data...\n",
      "Training model...\n",
      "Epoch 1, Train Loss: 0.7306, Val Loss: 1.3622\n",
      "Epoch 2, Train Loss: 0.7110, Val Loss: 1.4487\n",
      "Epoch 3, Train Loss: 0.6942, Val Loss: 1.4698\n",
      "Epoch 4, Train Loss: 0.6060, Val Loss: 1.0605\n",
      "Epoch 5, Train Loss: 0.5257, Val Loss: 1.7619\n",
      "Epoch 6, Train Loss: 0.4664, Val Loss: 1.5904\n",
      "Epoch 7, Train Loss: 0.4099, Val Loss: 1.7858\n",
      "Epoch 8, Train Loss: 0.3935, Val Loss: 1.0075\n",
      "Epoch 9, Train Loss: 0.3797, Val Loss: 1.3787\n",
      "Epoch 10, Train Loss: 0.3223, Val Loss: 0.7409\n",
      "Epoch 11, Train Loss: 0.3733, Val Loss: 0.7616\n",
      "Epoch 12, Train Loss: 0.2664, Val Loss: 0.6879\n",
      "Epoch 13, Train Loss: 0.2736, Val Loss: 0.6119\n",
      "Epoch 14, Train Loss: 0.2677, Val Loss: 0.4106\n",
      "Epoch 15, Train Loss: 0.2885, Val Loss: 0.2274\n",
      "Epoch 16, Train Loss: 0.2248, Val Loss: 0.3190\n",
      "Epoch 17, Train Loss: 0.2346, Val Loss: 0.1722\n",
      "Epoch 18, Train Loss: 0.2331, Val Loss: 0.3614\n",
      "Epoch 19, Train Loss: 0.1984, Val Loss: 0.1699\n",
      "Epoch 20, Train Loss: 0.1427, Val Loss: 0.2912\n",
      "Epoch 21, Train Loss: 0.1843, Val Loss: 1.3213\n",
      "Epoch 22, Train Loss: 0.0975, Val Loss: 0.4081\n",
      "Epoch 23, Train Loss: 0.1646, Val Loss: 0.4159\n",
      "Epoch 24, Train Loss: 0.0904, Val Loss: 0.1840\n",
      "Early stopping triggered after 24 epochs\n",
      "Done training, time: 134.01 秒\n",
      "Evaluating model...\n",
      "Results saved to D/cnn_lstm_results.txt\n",
      "Loading data...\n",
      "训练集分布：\n",
      "label\n",
      "0    1500\n",
      "1    1040\n",
      "Name: count, dtype: int64\n",
      "\n",
      "测试集分布：\n",
      "label\n",
      "0    5000\n",
      "1    5000\n",
      "Name: count, dtype: int64\n",
      "Preprocessing data...\n",
      "Training model...\n",
      "Epoch 1, Train Loss: 0.7303, Val Loss: 1.4939\n",
      "Epoch 2, Train Loss: 0.7014, Val Loss: 1.4930\n",
      "Epoch 3, Train Loss: 0.5700, Val Loss: 1.5820\n",
      "Epoch 4, Train Loss: 0.4650, Val Loss: 1.5866\n",
      "Epoch 5, Train Loss: 0.4461, Val Loss: 1.5845\n",
      "Epoch 6, Train Loss: 0.4095, Val Loss: 1.2275\n",
      "Epoch 7, Train Loss: 0.3809, Val Loss: 1.1424\n",
      "Epoch 8, Train Loss: 0.4379, Val Loss: 0.9684\n",
      "Epoch 9, Train Loss: 0.4209, Val Loss: 0.6057\n",
      "Epoch 10, Train Loss: 0.3423, Val Loss: 0.4905\n",
      "Epoch 11, Train Loss: 0.3529, Val Loss: 0.6873\n",
      "Epoch 12, Train Loss: 0.2721, Val Loss: 0.5493\n",
      "Epoch 13, Train Loss: 0.2423, Val Loss: 2.3901\n",
      "Epoch 14, Train Loss: 0.3441, Val Loss: 0.5809\n",
      "Epoch 15, Train Loss: 0.1923, Val Loss: 0.5321\n",
      "Early stopping triggered after 15 epochs\n",
      "Done training, time: 93.89 秒\n",
      "Evaluating model...\n",
      "Results saved to E/cnn_lstm_results.txt\n",
      "Loading data...\n",
      "训练集分布：\n",
      "label\n",
      "0    1500\n",
      "1    1040\n",
      "Name: count, dtype: int64\n",
      "\n",
      "测试集分布：\n",
      "label\n",
      "0    5000\n",
      "1    5000\n",
      "Name: count, dtype: int64\n",
      "Preprocessing data...\n",
      "Training model...\n",
      "Epoch 1, Train Loss: 0.7215, Val Loss: 1.4478\n",
      "Epoch 2, Train Loss: 0.6695, Val Loss: 1.9884\n",
      "Epoch 3, Train Loss: 0.4572, Val Loss: 1.7892\n",
      "Epoch 4, Train Loss: 0.4227, Val Loss: 1.4733\n",
      "Epoch 5, Train Loss: 0.3786, Val Loss: 1.0509\n",
      "Epoch 6, Train Loss: 0.3708, Val Loss: 1.4347\n",
      "Epoch 7, Train Loss: 0.3462, Val Loss: 1.4125\n",
      "Epoch 8, Train Loss: 0.3234, Val Loss: 1.4297\n",
      "Epoch 9, Train Loss: 0.2821, Val Loss: 1.6970\n",
      "Epoch 10, Train Loss: 0.2479, Val Loss: 1.5715\n",
      "Early stopping triggered after 10 epochs\n",
      "Done training, time: 65.38 秒\n",
      "Evaluating model...\n",
      "Results saved to F/cnn_lstm_results.txt\n",
      "Loading data...\n",
      "训练集分布：\n",
      "label\n",
      "0    1500\n",
      "1    1040\n",
      "Name: count, dtype: int64\n",
      "\n",
      "测试集分布：\n",
      "label\n",
      "0    5000\n",
      "1    5000\n",
      "Name: count, dtype: int64\n",
      "Preprocessing data...\n",
      "Training model...\n",
      "Epoch 1, Train Loss: 0.7206, Val Loss: 1.4952\n",
      "Epoch 2, Train Loss: 0.6834, Val Loss: 2.0132\n",
      "Epoch 3, Train Loss: 0.5235, Val Loss: 2.0994\n",
      "Epoch 4, Train Loss: 0.4666, Val Loss: 2.1448\n",
      "Epoch 5, Train Loss: 0.3727, Val Loss: 1.8626\n",
      "Epoch 6, Train Loss: 0.3156, Val Loss: 1.3090\n",
      "Epoch 7, Train Loss: 0.3014, Val Loss: 1.4353\n",
      "Epoch 8, Train Loss: 0.2692, Val Loss: 1.2415\n",
      "Epoch 9, Train Loss: 0.2698, Val Loss: 1.3831\n",
      "Epoch 10, Train Loss: 0.2797, Val Loss: 1.1399\n",
      "Epoch 11, Train Loss: 0.2434, Val Loss: 1.3213\n",
      "Epoch 12, Train Loss: 0.2457, Val Loss: 0.8117\n",
      "Epoch 13, Train Loss: 0.2445, Val Loss: 0.9601\n",
      "Epoch 14, Train Loss: 0.2488, Val Loss: 0.8294\n",
      "Epoch 15, Train Loss: 0.2362, Val Loss: 0.8141\n",
      "Epoch 16, Train Loss: 0.2390, Val Loss: 0.6905\n",
      "Epoch 17, Train Loss: 0.2379, Val Loss: 0.7652\n",
      "Epoch 18, Train Loss: 0.2237, Val Loss: 0.6664\n",
      "Epoch 19, Train Loss: 0.2095, Val Loss: 0.5638\n",
      "Epoch 20, Train Loss: 0.2209, Val Loss: 0.5803\n",
      "Epoch 21, Train Loss: 0.2038, Val Loss: 0.6680\n",
      "Epoch 22, Train Loss: 0.2168, Val Loss: 0.6139\n",
      "Epoch 23, Train Loss: 0.1999, Val Loss: 0.6865\n",
      "Epoch 24, Train Loss: 0.1899, Val Loss: 0.7230\n",
      "Early stopping triggered after 24 epochs\n",
      "Done training, time: 115.48 秒\n",
      "Evaluating model...\n",
      "Results saved to G/cnn_lstm_results.txt\n",
      "Loading data...\n",
      "训练集分布：\n",
      "label\n",
      "0    1500\n",
      "1    1040\n",
      "Name: count, dtype: int64\n",
      "\n",
      "测试集分布：\n",
      "label\n",
      "0    5000\n",
      "1    5000\n",
      "Name: count, dtype: int64\n",
      "Preprocessing data...\n",
      "Training model...\n",
      "Epoch 1, Train Loss: 0.7297, Val Loss: 1.4610\n",
      "Epoch 2, Train Loss: 0.7067, Val Loss: 1.4251\n",
      "Epoch 3, Train Loss: 0.6645, Val Loss: 1.4471\n",
      "Epoch 4, Train Loss: 0.4898, Val Loss: 0.6522\n",
      "Epoch 5, Train Loss: 0.4198, Val Loss: 0.8991\n",
      "Epoch 6, Train Loss: 0.3873, Val Loss: 0.4945\n",
      "Epoch 7, Train Loss: 0.2902, Val Loss: 0.6458\n",
      "Epoch 8, Train Loss: 0.2654, Val Loss: 1.5797\n",
      "Epoch 9, Train Loss: 0.2812, Val Loss: 0.9977\n",
      "Epoch 10, Train Loss: 0.2504, Val Loss: 0.2782\n",
      "Epoch 11, Train Loss: 0.2224, Val Loss: 0.4313\n",
      "Epoch 12, Train Loss: 0.2012, Val Loss: 2.2572\n",
      "Epoch 13, Train Loss: 0.2126, Val Loss: 1.2453\n",
      "Epoch 14, Train Loss: 0.1999, Val Loss: 1.5913\n",
      "Epoch 15, Train Loss: 0.1703, Val Loss: 1.0803\n",
      "Early stopping triggered after 15 epochs\n",
      "Done training, time: 89.47 秒\n",
      "Evaluating model...\n",
      "Results saved to R/cnn_lstm_results.txt\n",
      "Loading data...\n",
      "训练集分布：\n",
      "label\n",
      "0    1500\n",
      "1    1040\n",
      "Name: count, dtype: int64\n",
      "\n",
      "测试集分布：\n",
      "label\n",
      "0    5000\n",
      "1    5000\n",
      "Name: count, dtype: int64\n",
      "Preprocessing data...\n",
      "Training model...\n",
      "Epoch 1, Train Loss: 0.7361, Val Loss: 1.3478\n",
      "Epoch 2, Train Loss: 0.7112, Val Loss: 1.6992\n",
      "Epoch 3, Train Loss: 0.6958, Val Loss: 1.4713\n",
      "Epoch 4, Train Loss: 0.5863, Val Loss: 1.4653\n",
      "Epoch 5, Train Loss: 0.4601, Val Loss: 1.8621\n",
      "Epoch 6, Train Loss: 0.4744, Val Loss: 1.4195\n",
      "Early stopping triggered after 6 epochs\n",
      "Done training, time: 33.52 秒\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaylon/anaconda3/envs/llm/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to S/cnn_lstm_results.txt\n",
      "Loading data...\n",
      "训练集分布：\n",
      "label\n",
      "0    1500\n",
      "1    1040\n",
      "Name: count, dtype: int64\n",
      "\n",
      "测试集分布：\n",
      "label\n",
      "0    5000\n",
      "1    5000\n",
      "Name: count, dtype: int64\n",
      "Preprocessing data...\n",
      "Training model...\n",
      "Epoch 1, Train Loss: 0.7235, Val Loss: 1.4335\n",
      "Epoch 2, Train Loss: 0.7067, Val Loss: 1.4108\n",
      "Epoch 3, Train Loss: 0.5811, Val Loss: 1.6309\n",
      "Epoch 4, Train Loss: 0.4653, Val Loss: 1.6174\n",
      "Epoch 5, Train Loss: 0.4322, Val Loss: 1.2917\n",
      "Epoch 6, Train Loss: 0.3551, Val Loss: 0.4375\n",
      "Epoch 7, Train Loss: 0.3027, Val Loss: 0.7257\n",
      "Epoch 8, Train Loss: 0.3150, Val Loss: 0.6670\n",
      "Epoch 9, Train Loss: 0.2635, Val Loss: 0.4873\n",
      "Epoch 10, Train Loss: 0.2525, Val Loss: 0.1326\n",
      "Epoch 11, Train Loss: 0.2445, Val Loss: 0.4939\n",
      "Epoch 12, Train Loss: 0.1934, Val Loss: 0.5439\n",
      "Epoch 13, Train Loss: 0.2140, Val Loss: 0.2314\n",
      "Epoch 14, Train Loss: 0.2375, Val Loss: 0.1172\n",
      "Epoch 15, Train Loss: 0.2340, Val Loss: 0.4219\n",
      "Epoch 16, Train Loss: 0.1794, Val Loss: 0.1377\n",
      "Epoch 17, Train Loss: 0.1662, Val Loss: 0.7809\n",
      "Epoch 18, Train Loss: 0.1757, Val Loss: 0.1860\n",
      "Epoch 19, Train Loss: 0.1144, Val Loss: 0.3000\n",
      "Early stopping triggered after 19 epochs\n",
      "Done training, time: 94.13 秒\n",
      "Evaluating model...\n",
      "Results saved to W/cnn_lstm_results.txt\n"
     ]
    }
   ],
   "source": [
    "# 主函数\n",
    "def main():\n",
    "    unknown_types = [\"A\", \"B\", \"D\", \"E\", \"F\", \"G\", \"R\", \"S\", \"W\"]\n",
    "    for unknown_type in unknown_types:\n",
    "        # 创建保存结果的目录\n",
    "        if not os.path.exists(unknown_type):\n",
    "            os.makedirs(unknown_type)\n",
    "            \n",
    "        # 设置数据路径\n",
    "        train_self_path = '../../check/self/train_self_new.csv'\n",
    "        train_nonself_path = f'../../check/train/trainset_{unknown_type}_nonself.csv'\n",
    "        test_self_path = '../../check/self/test_self_new.csv'\n",
    "        test_nonself_path = '../../check/nonself/test_nonself.csv'\n",
    "        test_unknown_path = f'../../check/unknown/test/test{unknown_type}.csv'\n",
    "        train_unknown_path = f'../../check/unknown/train/train{unknown_type}.csv'\n",
    "        \n",
    "        # 加载数据\n",
    "        print(\"Loading data...\")\n",
    "        train_data, test_data, unknown = load_data(train_self_path, train_nonself_path, test_self_path, test_nonself_path, test_unknown_path, train_unknown_path)\n",
    "        \n",
    "        # 预处理数据\n",
    "        print(\"Preprocessing data...\")\n",
    "        X_train, y_train, X_test, y_test = preprocess_data(train_data, test_data)\n",
    "        \n",
    "        # 创建数据集和数据加载器\n",
    "        # 将训练集分为训练集和验证集（80%训练，20%验证）\n",
    "        train_size = int(0.8 * len(X_train))\n",
    "        val_size = len(X_train) - train_size\n",
    "        \n",
    "        X_train_split, X_val = X_train[:train_size], X_train[train_size:]\n",
    "        y_train_split, y_val = y_train[:train_size], y_train[train_size:]\n",
    "        \n",
    "        train_dataset = IntrusionDataset(X_train_split, y_train_split)\n",
    "        val_dataset = IntrusionDataset(X_val, y_val)\n",
    "        test_dataset = IntrusionDataset(X_test, y_test)\n",
    "        \n",
    "        batch_size = 64\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "        \n",
    "        # 创建模型\n",
    "        input_size = X_train.shape[2]  # 特征维度\n",
    "        model = CNN_LSTM(input_size)\n",
    "        \n",
    "        # 计算正样本权重\n",
    "        pos_weight = torch.tensor([(y_train == 0).sum() / (y_train == 1).sum()])\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
    "        \n",
    "        # 使用带权重衰减的优化器\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "        \n",
    "        # 添加学习率调度器\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.1)\n",
    "        \n",
    "        # 训练模型\n",
    "        print(\"Training model...\")\n",
    "        start_time = time.time()\n",
    "        model, train_losses, val_losses = train_model(\n",
    "            model, \n",
    "            train_loader, \n",
    "            val_loader, \n",
    "            criterion, \n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            num_epochs=30, \n",
    "            patience=5\n",
    "        )\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"Done training, time: {training_time:.2f} 秒\")\n",
    "        \n",
    "        # 评估模型\n",
    "        print(\"Evaluating model...\")\n",
    "        accuracy, precision, recall, f1, conf_matrix = evaluate_model(model, test_loader)\n",
    "        unknown_coverage = evaluate_unknown_coverage(model, unknown)\n",
    "        test_self_data = test_data[test_data['label'] == 0]\n",
    "        false_positive_rate = evaluate_false_positive_rate(model, test_self_data)\n",
    "        \n",
    "        # 保存结果\n",
    "        results = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'unknown_coverage': unknown_coverage,\n",
    "            'false_positive_rate': false_positive_rate,\n",
    "            'training_time': training_time,\n",
    "            'confusion_matrix': conf_matrix.tolist()\n",
    "        }\n",
    "        \n",
    "        # 将结果保存为文本文件\n",
    "        with open(f'{unknown_type}/cnn_lstm_results.txt', 'w') as f:\n",
    "            for key, value in results.items():\n",
    "                if key != 'confusion_matrix':\n",
    "                    f.write(f\"{key}: {value}\\n\")\n",
    "            f.write(f\"confusion_matrix:\\n{conf_matrix}\\n\")\n",
    "        \n",
    "        print(f\"Results saved to {unknown_type}/cnn_lstm_results.txt\")\n",
    "\n",
    "        # # 计算最佳阈值\n",
    "        # validation_data = pd.concat([train_data, test_data], axis=0).sample(frac=0.2, random_state=42)\n",
    "        # X_val_data = validation_data.drop('label', axis=1).values\n",
    "        # y_val_data = validation_data['label'].values\n",
    "        # X_val_seq = X_val_data.reshape(X_val_data.shape[0], 1, X_val_data.shape[1])\n",
    "        # best_result = find_optimal_threshold(model, X_val_seq, y_val_data, unknown, test_self_data)\n",
    "        # best_threshold, best_f1, best_unknown_cov, best_fpr, best_score = best_result\n",
    "        # with open(f'{unknown_type}/best_threshold_results.txt', 'w') as f:\n",
    "        #     f.write(f\"Best Threshold: {best_threshold:.6f}\\n\")\n",
    "        #     f.write(f\"F1 Score at Best Threshold: {best_f1:.4f}\\n\")\n",
    "        #     f.write(f\"Unknown Coverage at Best Threshold: {best_unknown_cov:.4f}\\n\")\n",
    "        #     f.write(f\"False Positive Rate at Best Threshold: {best_fpr:.4f}\\n\")\n",
    "        #     f.write(f\"Combined Score: {best_score:.4f}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
