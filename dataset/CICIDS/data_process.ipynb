{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1353594/2464509963.py:4: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_2 = pd.read_csv(\"origin/Friday-16-02-2018_TrafficForML_CICFlowMeter.csv\")\n",
      "/tmp/ipykernel_1353594/2464509963.py:6: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_4 = pd.read_csv(\"origin/Thursday-01-03-2018_TrafficForML_CICFlowMeter.csv\")\n",
      "/tmp/ipykernel_1353594/2464509963.py:10: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_8 = pd.read_csv(\"origin/Wednesday-28-02-2018_TrafficForML_CICFlowMeter.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "列数: 80\n"
     ]
    }
   ],
   "source": [
    "# 载入原始数据集\n",
    "import pandas as pd\n",
    "data_1 = pd.read_csv(\"origin/Friday-02-03-2018_TrafficForML_CICFlowMeter.csv\")\n",
    "data_2 = pd.read_csv(\"origin/Friday-16-02-2018_TrafficForML_CICFlowMeter.csv\")\n",
    "data_3 = pd.read_csv(\"origin/Friday-23-02-2018_TrafficForML_CICFlowMeter.csv\")\n",
    "data_4 = pd.read_csv(\"origin/Thursday-01-03-2018_TrafficForML_CICFlowMeter.csv\")\n",
    "data_5 = pd.read_csv(\"origin/Thursday-15-02-2018_TrafficForML_CICFlowMeter.csv\")\n",
    "data_6 = pd.read_csv(\"origin/Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv\")\n",
    "data_7 = pd.read_csv(\"origin/Wednesday-21-02-2018_TrafficForML_CICFlowMeter.csv\")\n",
    "data_8 = pd.read_csv(\"origin/Wednesday-28-02-2018_TrafficForML_CICFlowMeter.csv\")\n",
    "data = pd.concat([data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8])\n",
    "print(\"列数:\", len(data.columns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7235679\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Timestamp'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 删除异常数据\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m data[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/pandas/core/frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[1;32m   5582\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m   5583\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   5584\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m   5585\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   5586\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m   5587\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[1;32m   5588\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m   5589\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/pandas/core/generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/pandas/core/generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Timestamp'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# 删除异常数据\n",
    "\n",
    "data = data.drop(['Timestamp'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['Label'] != 'Label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7235620\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.sample(n=20000, random_state=42)\n",
    "\n",
    "test_data = data[~data.index.isin(train_data.index)].sample(n=20000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "训练集标签分布:\n",
      "\n",
      "各标签数量及占比:\n",
      "Benign: 14939 (74.69%)\n",
      "DDOS attack-HOIC: 1946 (9.73%)\n",
      "DoS attacks-Hulk: 1304 (6.52%)\n",
      "Bot: 834 (4.17%)\n",
      "Infilteration: 440 (2.20%)\n",
      "DoS attacks-SlowHTTPTest: 379 (1.90%)\n",
      "DoS attacks-GoldenEye: 125 (0.62%)\n",
      "DoS attacks-Slowloris: 26 (0.13%)\n",
      "DDOS attack-LOIC-UDP: 4 (0.02%)\n",
      "Brute Force -Web: 2 (0.01%)\n",
      "SQL Injection: 1 (0.01%)\n",
      "\n",
      "测试集标签分布:\n",
      "\n",
      "各标签数量及占比:\n",
      "Benign: 14978 (74.89%)\n",
      "DDOS attack-HOIC: 1927 (9.63%)\n",
      "DoS attacks-Hulk: 1250 (6.25%)\n",
      "Bot: 815 (4.08%)\n",
      "Infilteration: 444 (2.22%)\n",
      "DoS attacks-SlowHTTPTest: 424 (2.12%)\n",
      "DoS attacks-GoldenEye: 128 (0.64%)\n",
      "DoS attacks-Slowloris: 28 (0.14%)\n",
      "DDOS attack-LOIC-UDP: 5 (0.03%)\n",
      "Brute Force -XSS: 1 (0.01%)\n"
     ]
    }
   ],
   "source": [
    "# 打印标签分布\n",
    "print(\"\\n训练集标签分布:\")\n",
    "label_counts = train_data['Label'].value_counts()\n",
    "total = len(train_data)\n",
    "\n",
    "print(\"\\n各标签数量及占比:\")\n",
    "for label, count in label_counts.items():\n",
    "    percentage = count/total * 100\n",
    "    print(f\"{label}: {count} ({percentage:.2f}%)\")\n",
    "    \n",
    "print(\"\\n测试集标签分布:\")\n",
    "label_counts = test_data['Label'].value_counts()\n",
    "total = len(test_data)\n",
    "\n",
    "print(\"\\n各标签数量及占比:\")\n",
    "for label, count in label_counts.items():\n",
    "    percentage = count/total * 100\n",
    "    print(f\"{label}: {count} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正常流量数量: 5444525\n",
      "DDoS攻击数量: 687742\n",
      "DoS攻击数量: 654300\n",
      "Bot攻击数量: 286191\n",
      "渗透攻击数量: 161934\n",
      "暴力破解攻击数量: 841\n",
      "SQL注入攻击数量: 87\n"
     ]
    }
   ],
   "source": [
    "# 正常流量\n",
    "benign_data = data[data['Label'] == 'Benign']\n",
    "print(\"正常流量数量:\", len(benign_data))\n",
    "\n",
    "# DDoS攻击\n",
    "ddos_hoic_data = data[data['Label'] == 'DDOS attack-HOIC'] \n",
    "ddos_loic_data = data[data['Label'] == 'DDOS attack-LOIC-UDP']\n",
    "ddos = pd.concat([ddos_hoic_data, ddos_loic_data])\n",
    "print(\"DDoS攻击数量:\", len(ddos))\n",
    "\n",
    "# DoS攻击\n",
    "dos_hulk_data = data[data['Label'] == 'DoS attacks-Hulk']\n",
    "dos_slowhttp_data = data[data['Label'] == 'DoS attacks-SlowHTTPTest']\n",
    "dos_golden_data = data[data['Label'] == 'DoS attacks-GoldenEye']\n",
    "dos_slowloris_data = data[data['Label'] == 'DoS attacks-Slowloris']\n",
    "dos = pd.concat([dos_hulk_data, dos_slowhttp_data, dos_golden_data, dos_slowloris_data])\n",
    "print(\"DoS攻击数量:\", len(dos))\n",
    "\n",
    "# Bot攻击\n",
    "bot_data = data[data['Label'] == 'Bot']\n",
    "bot = pd.concat([bot_data])\n",
    "print(\"Bot攻击数量:\", len(bot))\n",
    "\n",
    "# 渗透攻击\n",
    "infilteration_data = data[data['Label'] == 'Infilteration']\n",
    "infilteration = pd.concat([infilteration_data])\n",
    "print(\"渗透攻击数量:\", len(infilteration))\n",
    "\n",
    "# 暴力破解攻击\n",
    "bruteforce_web_data = data[data['Label'] == 'Brute Force -Web']\n",
    "bruteforce_xss_data = data[data['Label'] == 'Brute Force -XSS']\n",
    "bruteforce = pd.concat([bruteforce_web_data, bruteforce_xss_data])\n",
    "print(\"暴力破解攻击数量:\", len(bruteforce))\n",
    "\n",
    "# SQL注入攻击\n",
    "sql_injection_data = data[data['Label'] == 'SQL Injection']\n",
    "sql_injection = pd.concat([sql_injection_data])\n",
    "print(\"SQL注入攻击数量:\", len(sql_injection))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_train = benign_data.sample(n=20000, random_state=42)\n",
    "benign_test = benign_data[~benign_data.index.isin(benign_train.index)].sample(n=100000, random_state=42)\n",
    "\n",
    "ddos_train = ddos.sample(n=500, random_state=42)\n",
    "ddos_test = ddos[~ddos.index.isin(ddos_train.index)].sample(n=10000, random_state=42)\n",
    "\n",
    "dos_train = dos.sample(n=500, random_state=42)\n",
    "dos_test = dos[~dos.index.isin(dos_train.index)].sample(n=10000, random_state=42)\n",
    "\n",
    "bot_train = bot.sample(n=300, random_state=42)\n",
    "bot_test = bot[~bot.index.isin(bot_train.index)].sample(n=10000, random_state=42)\n",
    "\n",
    "infilteration_train = infilteration.sample(n=300, random_state=42)\n",
    "infilteration_test = infilteration[~infilteration.index.isin(infilteration_train.index)].sample(n=10000, random_state=42)\n",
    "\n",
    "bruteforce_train = bruteforce.sample(n=100, random_state=42)\n",
    "bruteforce_test = bruteforce[~bruteforce.index.isin(bruteforce_train.index)].sample(n=700, random_state=42)\n",
    "\n",
    "sql_injection_train = sql_injection.sample(n=20, random_state=42)\n",
    "sql_injection_test = sql_injection[~sql_injection.index.isin(sql_injection_train.index)].sample(n=67, random_state=42)\n",
    "\n",
    "train_data = pd.concat([benign_data, ddos_train, dos_train, bot_train, infilteration_train, bruteforce_train, sql_injection_train])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集测试集\n",
    "train_data = pd.concat([benign_data, dos, ddos, bruteforce, sql_injection])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始数据集处理\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "train_data['protocol_type'] = label_encoder.fit_transform(train_data['protocol_type'])\n",
    "test_data['protocol_type'] = label_encoder.fit_transform(test_data['protocol_type'])\n",
    "train_data['service'] = label_encoder.fit_transform(train_data['service'])\n",
    "test_data['service'] = label_encoder.fit_transform(test_data['service'])\n",
    "train_data['flag'] = label_encoder.fit_transform(train_data['flag'])\n",
    "test_data['flag'] = label_encoder.fit_transform(test_data['flag'])\n",
    "\n",
    "# 保存修改后的数据\n",
    "train_data.to_csv(\"train_data.csv\",index=False)\n",
    "test_data.to_csv(\"test_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_self:\n",
      "67343\n",
      "test_self:\n",
      "9711\n",
      "train_nonself:\n",
      "58630\n",
      "test_nonself:\n",
      "12833\n"
     ]
    }
   ],
   "source": [
    "# 处理自体数据\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "test_self = test_data[test_data['class'] == 'normal'].drop(['class', 'num'], axis=1)\n",
    "train_self = train_data[train_data['class'] == 'normal'].drop(['class', 'num'], axis=1)\n",
    "\n",
    "train_self = pd.DataFrame(scaler.fit_transform(train_self).round(5))\n",
    "test_self = pd.DataFrame(scaler.fit_transform(test_self).round(5))\n",
    "print('train_self:')\n",
    "print(len(train_self))\n",
    "print('test_self:')\n",
    "print(len(test_self))\n",
    "test_self.to_csv(\"self/test_self.csv\",index=False)\n",
    "train_self.to_csv(\"self/train_self.csv\",index=False)\n",
    "\n",
    "# 处理非自体数据\n",
    "test_nonself = test_data[test_data['class'] != 'normal'].drop(['class', 'num'], axis=1)\n",
    "train_nonself = train_data[train_data['class'] != 'normal'].drop(['class', 'num'], axis=1)\n",
    "train_nonself = pd.DataFrame(scaler.fit_transform(train_nonself).round(5))\n",
    "test_nonself = pd.DataFrame(scaler.fit_transform(test_nonself).round(5))\n",
    "print('train_nonself:')\n",
    "print(len(train_nonself))\n",
    "print('test_nonself:')\n",
    "print(len(test_nonself))\n",
    "test_nonself.to_csv(\"nonself/test_nonself.csv\",index=False)\n",
    "train_nonself.to_csv(\"nonself/train_nonself.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集中的攻击类型:\n",
      "['normal' 'neptune' 'warezclient' 'ipsweep' 'portsweep' 'teardrop' 'nmap'\n",
      " 'satan' 'smurf' 'pod' 'back' 'guess_passwd' 'ftp_write' 'multihop'\n",
      " 'rootkit' 'buffer_overflow' 'imap' 'warezmaster' 'phf' 'land'\n",
      " 'loadmodule' 'spy' 'perl']\n",
      "\n",
      "测试集中的攻击类型:\n",
      "['neptune' 'normal' 'saint' 'mscan' 'guess_passwd' 'smurf' 'apache2'\n",
      " 'satan' 'buffer_overflow' 'back' 'warezmaster' 'snmpgetattack'\n",
      " 'processtable' 'pod' 'httptunnel' 'nmap' 'ps' 'snmpguess' 'ipsweep'\n",
      " 'mailbomb' 'portsweep' 'multihop' 'named' 'sendmail' 'loadmodule' 'xterm'\n",
      " 'worm' 'teardrop' 'rootkit' 'xlock' 'perl' 'land' 'xsnoop' 'sqlattack'\n",
      " 'ftp_write' 'imap' 'udpstorm' 'phf']\n"
     ]
    }
   ],
   "source": [
    "# 打印训练集、测试集中所有class的种类\n",
    "print(\"训练集中的攻击类型:\")\n",
    "print(train_data['class'].unique())\n",
    "print(\"\\n测试集中的攻击类型:\") \n",
    "print(test_data['class'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 测试集数据攻击分类\n",
    "# test_back = test_data[test_data['class'] == 'back'].drop(['class', 'num'], axis=1)\n",
    "# test_back = pd.DataFrame(scaler.fit_transform(test_back).round(5))\n",
    "\n",
    "# test_buffer_overflow = test_data[test_data['class'] == 'buffer_overflow'].drop(['class', 'num'], axis=1) \n",
    "# test_buffer_overflow = pd.DataFrame(scaler.fit_transform(test_buffer_overflow).round(5))\n",
    "\n",
    "# test_ftp_write = test_data[test_data['class'] == 'ftp_write'].drop(['class', 'num'], axis=1)  \n",
    "# test_ftp_write = pd.DataFrame(scaler.fit_transform(test_ftp_write).round(5))\n",
    "\n",
    "# test_guess_passwd = test_data[test_data['class'] == 'guess_passwd'].drop(['class', 'num'], axis=1)\n",
    "# test_guess_passwd = pd.DataFrame(scaler.fit_transform(test_guess_passwd).round(5))\n",
    "\n",
    "# test_httptunnel = test_data[test_data['class'] == 'httptunnel'].drop(['class', 'num'], axis=1)\n",
    "# test_httptunnel = pd.DataFrame(scaler.fit_transform(test_httptunnel).round(5))\n",
    "\n",
    "# test_imap = test_data[test_data['class'] == 'imap'].drop(['class', 'num'], axis=1)\n",
    "# test_imap = pd.DataFrame(scaler.fit_transform(test_imap).round(5))\n",
    "\n",
    "# test_ipsweep = test_data[test_data['class'] == 'ipsweep'].drop(['class', 'num'], axis=1)\n",
    "# test_ipsweep = pd.DataFrame(scaler.fit_transform(test_ipsweep).round(5))\n",
    "\n",
    "# test_land = test_data[test_data['class'] == 'land'].drop(['class', 'num'], axis=1)\n",
    "# test_land = pd.DataFrame(scaler.fit_transform(test_land).round(5))\n",
    "\n",
    "# test_loadmodule = test_data[test_data['class'] == 'loadmodule'].drop(['class', 'num'], axis=1)\n",
    "# test_loadmodule = pd.DataFrame(scaler.fit_transform(test_loadmodule).round(5))\n",
    "\n",
    "# test_mailbomb = test_data[test_data['class'] == 'mailbomb'].drop(['class', 'num'], axis=1)\n",
    "# test_mailbomb = pd.DataFrame(scaler.fit_transform(test_mailbomb).round(5))\n",
    "\n",
    "# test_mscan = test_data[test_data['class'] == 'mscan'].drop(['class', 'num'], axis=1)\n",
    "# test_mscan = pd.DataFrame(scaler.fit_transform(test_mscan).round(5))\n",
    "\n",
    "# test_multihop = test_data[test_data['class'] == 'multihop'].drop(['class', 'num'], axis=1)\n",
    "# test_multihop = pd.DataFrame(scaler.fit_transform(test_multihop).round(5))\n",
    "\n",
    "# test_named = test_data[test_data['class'] == 'named'].drop(['class', 'num'], axis=1)\n",
    "# test_named = pd.DataFrame(scaler.fit_transform(test_named).round(5))\n",
    "\n",
    "# test_neptune = test_data[test_data['class'] == 'neptune'].drop(['class', 'num'], axis=1)\n",
    "# test_neptune = pd.DataFrame(scaler.fit_transform(test_neptune).round(5))\n",
    "\n",
    "# test_nmap = test_data[test_data['class'] == 'nmap'].drop(['class', 'num'], axis=1)\n",
    "# test_nmap = pd.DataFrame(scaler.fit_transform(test_nmap).round(5))\n",
    "\n",
    "# test_perl = test_data[test_data['class'] == 'perl'].drop(['class', 'num'], axis=1)\n",
    "# test_perl = pd.DataFrame(scaler.fit_transform(test_perl).round(5))\n",
    "\n",
    "# test_phf = test_data[test_data['class'] == 'phf'].drop(['class', 'num'], axis=1)\n",
    "# test_phf = pd.DataFrame(scaler.fit_transform(test_phf).round(5))\n",
    "\n",
    "# test_pod = test_data[test_data['class'] == 'pod'].drop(['class', 'num'], axis=1)\n",
    "# test_pod = pd.DataFrame(scaler.fit_transform(test_pod).round(5))\n",
    "\n",
    "# test_portsweep = test_data[test_data['class'] == 'portsweep'].drop(['class', 'num'], axis=1)\n",
    "# test_portsweep = pd.DataFrame(scaler.fit_transform(test_portsweep).round(5))\n",
    "\n",
    "# test_processtable = test_data[test_data['class'] == 'processtable'].drop(['class', 'num'], axis=1)\n",
    "# test_processtable = pd.DataFrame(scaler.fit_transform(test_processtable).round(5))\n",
    "\n",
    "# test_rootkit = test_data[test_data['class'] == 'rootkit'].drop(['class', 'num'], axis=1)\n",
    "# test_rootkit = pd.DataFrame(scaler.fit_transform(test_rootkit).round(5))\n",
    "\n",
    "# test_saint = test_data[test_data['class'] == 'saint'].drop(['class', 'num'], axis=1)\n",
    "# test_saint = pd.DataFrame(scaler.fit_transform(test_saint).round(5))\n",
    "\n",
    "# test_satan = test_data[test_data['class'] == 'satan'].drop(['class', 'num'], axis=1)\n",
    "# test_satan = pd.DataFrame(scaler.fit_transform(test_satan).round(5))\n",
    "\n",
    "# test_sendmail = test_data[test_data['class'] == 'sendmail'].drop(['class', 'num'], axis=1)\n",
    "# test_sendmail = pd.DataFrame(scaler.fit_transform(test_sendmail).round(5))\n",
    "\n",
    "# test_snmpgetattack = test_data[test_data['class'] == 'snmpgetattack'].drop(['class', 'num'], axis=1)\n",
    "# test_snmpgetattack = pd.DataFrame(scaler.fit_transform(test_snmpgetattack).round(5))\n",
    "\n",
    "# test_snmpguess = test_data[test_data['class'] == 'snmpguess'].drop(['class', 'num'], axis=1)\n",
    "# test_snmpguess = pd.DataFrame(scaler.fit_transform(test_snmpguess).round(5))\n",
    "\n",
    "# test_sqlattack = test_data[test_data['class'] == 'sqlattack'].drop(['class', 'num'], axis=1)\n",
    "# test_sqlattack = pd.DataFrame(scaler.fit_transform(test_sqlattack).round(5))\n",
    "\n",
    "# test_teardrop = test_data[test_data['class'] == 'teardrop'].drop(['class', 'num'], axis=1)\n",
    "# test_teardrop = pd.DataFrame(scaler.fit_transform(test_teardrop).round(5))\n",
    "\n",
    "# test_udpstorm = test_data[test_data['class'] == 'udpstorm'].drop(['class', 'num'], axis=1)\n",
    "# test_udpstorm = pd.DataFrame(scaler.fit_transform(test_udpstorm).round(5))\n",
    "\n",
    "# test_warezmaster = test_data[test_data['class'] == 'warezmaster'].drop(['class', 'num'], axis=1)\n",
    "# test_warezmaster = pd.DataFrame(scaler.fit_transform(test_warezmaster).round(5))\n",
    "\n",
    "# test_worm = test_data[test_data['class'] == 'worm'].drop(['class', 'num'], axis=1)\n",
    "# test_worm = pd.DataFrame(scaler.fit_transform(test_worm).round(5))\n",
    "\n",
    "# test_xlock = test_data[test_data['class'] == 'xlock'].drop(['class', 'num'], axis=1)\n",
    "# test_xlock = pd.DataFrame(scaler.fit_transform(test_xlock).round(5))\n",
    "\n",
    "# test_xsnoop = test_data[test_data['class'] == 'xsnoop'].drop(['class', 'num'], axis=1)\n",
    "# test_xsnoop = pd.DataFrame(scaler.fit_transform(test_xsnoop).round(5))\n",
    "\n",
    "# test_xterm = test_data[test_data['class'] == 'xterm'].drop(['class', 'num'], axis=1)\n",
    "# test_xterm = pd.DataFrame(scaler.fit_transform(test_xterm).round(5))\n",
    "\n",
    "# test_apache2 = test_data[test_data['class'] == 'apache2'].drop(['class', 'num'], axis=1)\n",
    "# test_apache2 = pd.DataFrame(scaler.fit_transform(test_apache2).round(5))\n",
    "\n",
    "# test_ps = test_data[test_data['class'] == 'ps'].drop(['class', 'num'], axis=1)\n",
    "# test_ps = pd.DataFrame(scaler.fit_transform(test_ps).round(5))\n",
    "\n",
    "# test_smurf = test_data[test_data['class'] == 'smurf'].drop(['class', 'num'], axis=1)\n",
    "# test_smurf = pd.DataFrame(scaler.fit_transform(test_smurf).round(5))\n",
    "\n",
    "# total_len = len(test_nonself)\n",
    "\n",
    "# # 打印每一类长度和比例\n",
    "# print(f\"test_Total: {len(test_nonself)}\")\n",
    "# print(f\"test_back: {len(test_back)} ({len(test_back) / total_len:.2%})\")\n",
    "# print(f\"test_buffer_overflow: {len(test_buffer_overflow)} ({len(test_buffer_overflow) / total_len:.2%})\")\n",
    "# print(f\"test_ftp_write: {len(test_ftp_write)} ({len(test_ftp_write) / total_len:.2%})\")\n",
    "# print(f\"test_guess_passwd: {len(test_guess_passwd)} ({len(test_guess_passwd) / total_len:.2%})\")\n",
    "# print(f\"test_httptunnel: {len(test_httptunnel)} ({len(test_httptunnel) / total_len:.2%})\")\n",
    "# print(f\"test_imap: {len(test_imap)} ({len(test_imap) / total_len:.2%})\")\n",
    "# print(f\"test_ipsweep: {len(test_ipsweep)} ({len(test_ipsweep) / total_len:.2%})\")\n",
    "# print(f\"test_land: {len(test_land)} ({len(test_land) / total_len:.2%})\")\n",
    "# print(f\"test_loadmodule: {len(test_loadmodule)} ({len(test_loadmodule) / total_len:.2%})\")\n",
    "# print(f\"test_mailbomb: {len(test_mailbomb)} ({len(test_mailbomb) / total_len:.2%})\")\n",
    "# print(f\"test_mscan: {len(test_mscan)} ({len(test_mscan) / total_len:.2%})\")\n",
    "# print(f\"test_multihop: {len(test_multihop)} ({len(test_multihop) / total_len:.2%})\")\n",
    "# print(f\"test_named: {len(test_named)} ({len(test_named) / total_len:.2%})\")\n",
    "# print(f\"test_neptune: {len(test_neptune)} ({len(test_neptune) / total_len:.2%})\")\n",
    "# print(f\"test_nmap: {len(test_nmap)} ({len(test_nmap) / total_len:.2%})\")\n",
    "# print(f\"test_perl: {len(test_perl)} ({len(test_perl) / total_len:.2%})\")\n",
    "# print(f\"test_phf: {len(test_phf)} ({len(test_phf) / total_len:.2%})\")\n",
    "# print(f\"test_pod: {len(test_pod)} ({len(test_pod) / total_len:.2%})\")\n",
    "# print(f\"test_portsweep: {len(test_portsweep)} ({len(test_portsweep) / total_len:.2%})\")\n",
    "# print(f\"test_processtable: {len(test_processtable)} ({len(test_processtable) / total_len:.2%})\")\n",
    "# print(f\"test_rootkit: {len(test_rootkit)} ({len(test_rootkit) / total_len:.2%})\")\n",
    "# print(f\"test_saint: {len(test_saint)} ({len(test_saint) / total_len:.2%})\")\n",
    "# print(f\"test_satan: {len(test_satan)} ({len(test_satan) / total_len:.2%})\")\n",
    "# print(f\"test_sendmail: {len(test_sendmail)} ({len(test_sendmail) / total_len:.2%})\")\n",
    "# print(f\"test_snmpgetattack: {len(test_snmpgetattack)} ({len(test_snmpgetattack) / total_len:.2%})\")\n",
    "# print(f\"test_snmpguess: {len(test_snmpguess)} ({len(test_snmpguess) / total_len:.2%})\")\n",
    "# print(f\"test_sqlattack: {len(test_sqlattack)} ({len(test_sqlattack) / total_len:.2%})\")\n",
    "# print(f\"test_teardrop: {len(test_teardrop)} ({len(test_teardrop) / total_len:.2%})\")\n",
    "# print(f\"test_udpstorm: {len(test_udpstorm)} ({len(test_udpstorm) / total_len:.2%})\")\n",
    "# print(f\"test_warezmaster: {len(test_warezmaster)} ({len(test_warezmaster) / total_len:.2%})\")\n",
    "# print(f\"test_worm: {len(test_worm)} ({len(test_worm) / total_len:.2%})\")\n",
    "# print(f\"test_xlock: {len(test_xlock)} ({len(test_xlock) / total_len:.2%})\")\n",
    "# print(f\"test_xsnoop: {len(test_xsnoop)} ({len(test_xsnoop) / total_len:.2%})\")\n",
    "# print(f\"test_xterm: {len(test_xterm)} ({len(test_xterm) / total_len:.2%})\")\n",
    "# print(f\"test_apache2: {len(test_apache2)} ({len(test_apache2) / total_len:.2%})\")\n",
    "# print(f\"test_ps: {len(test_ps)} ({len(test_ps) / total_len:.2%})\")\n",
    "# print(f\"test_smurf: {len(test_smurf)} ({len(test_smurf) / total_len:.2%})\")\n",
    "# print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Total: 58630\n",
      "train_neptune: 41214 (70.30%)\n",
      "train_warezclient: 890 (1.52%)\n",
      "train_ipsweep: 3599 (6.14%)\n",
      "train_portsweep: 2931 (5.00%)\n",
      "train_teardrop: 892 (1.52%)\n",
      "train_nmap: 1493 (2.55%)\n",
      "train_guess_passwd: 53 (0.09%)\n",
      "train_ftp_write: 8 (0.01%)\n",
      "train_multihop: 7 (0.01%)\n",
      "train_satan: 3633 (6.20%)\n",
      "train_smurf: 2646 (4.51%)\n",
      "train_pod: 201 (0.34%)\n",
      "train_back: 956 (1.63%)\n",
      "train_rootkit: 10 (0.02%)\n",
      "train_buffer_overflow: 30 (0.05%)\n",
      "train_phf: 4 (0.01%)\n",
      "train_land: 18 (0.03%)\n",
      "train_imap: 11 (0.02%)\n",
      "train_warezmaster: 20 (0.03%)\n",
      "train_loadmodule: 9 (0.02%)\n",
      "train_spy: 2 (0.00%)\n",
      "train_perl: 3 (0.01%)\n"
     ]
    }
   ],
   "source": [
    "# 训练集数据攻击分类\n",
    "train_neptune = train_data[train_data['class'] == 'neptune'].drop(['class', 'num'], axis=1)\n",
    "train_neptune = pd.DataFrame(scaler.fit_transform(train_neptune).round(5))\n",
    "\n",
    "train_warezclient = train_data[train_data['class'] == 'warezclient'].drop(['class', 'num'], axis=1)\n",
    "train_warezclient = pd.DataFrame(scaler.fit_transform(train_warezclient).round(5))\n",
    "\n",
    "train_ipsweep = train_data[train_data['class'] == 'ipsweep'].drop(['class', 'num'], axis=1)\n",
    "train_ipsweep = pd.DataFrame(scaler.fit_transform(train_ipsweep).round(5))\n",
    "\n",
    "train_portsweep = train_data[train_data['class'] == 'portsweep'].drop(['class', 'num'], axis=1)\n",
    "train_portsweep = pd.DataFrame(scaler.fit_transform(train_portsweep).round(5))\n",
    "\n",
    "train_teardrop = train_data[train_data['class'] == 'teardrop'].drop(['class', 'num'], axis=1)\n",
    "train_teardrop = pd.DataFrame(scaler.fit_transform(train_teardrop).round(5))\n",
    "\n",
    "train_nmap = train_data[train_data['class'] == 'nmap'].drop(['class', 'num'], axis=1)\n",
    "train_nmap = pd.DataFrame(scaler.fit_transform(train_nmap).round(5))\n",
    "\n",
    "train_guess_passwd = train_data[train_data['class'] == 'guess_passwd'].drop(['class', 'num'], axis=1)\n",
    "train_guess_passwd = pd.DataFrame(scaler.fit_transform(train_guess_passwd).round(5))\n",
    "\n",
    "train_ftp_write = train_data[train_data['class'] == 'ftp_write'].drop(['class', 'num'], axis=1)\n",
    "train_ftp_write = pd.DataFrame(scaler.fit_transform(train_ftp_write).round(5))\n",
    "\n",
    "train_multihop = train_data[train_data['class'] == 'multihop'].drop(['class', 'num'], axis=1)\n",
    "train_multihop = pd.DataFrame(scaler.fit_transform(train_multihop).round(5))\n",
    "\n",
    "train_satan = train_data[train_data['class'] == 'satan'].drop(['class', 'num'], axis=1)\n",
    "train_satan = pd.DataFrame(scaler.fit_transform(train_satan).round(5))\n",
    "\n",
    "train_smurf = train_data[train_data['class'] == 'smurf'].drop(['class', 'num'], axis=1)\n",
    "train_smurf = pd.DataFrame(scaler.fit_transform(train_smurf).round(5))\n",
    "\n",
    "train_pod = train_data[train_data['class'] == 'pod'].drop(['class', 'num'], axis=1)\n",
    "train_pod = pd.DataFrame(scaler.fit_transform(train_pod).round(5))\n",
    "\n",
    "train_back = train_data[train_data['class'] == 'back'].drop(['class', 'num'], axis=1)\n",
    "train_back = pd.DataFrame(scaler.fit_transform(train_back).round(5))\n",
    "\n",
    "train_rootkit = train_data[train_data['class'] == 'rootkit'].drop(['class', 'num'], axis=1)\n",
    "train_rootkit = pd.DataFrame(scaler.fit_transform(train_rootkit).round(5))\n",
    "\n",
    "train_buffer_overflow = train_data[train_data['class'] == 'buffer_overflow'].drop(['class', 'num'], axis=1)\n",
    "train_buffer_overflow = pd.DataFrame(scaler.fit_transform(train_buffer_overflow).round(5))\n",
    "\n",
    "train_phf = train_data[train_data['class'] == 'phf'].drop(['class', 'num'], axis=1)\n",
    "train_phf = pd.DataFrame(scaler.fit_transform(train_phf).round(5))\n",
    "\n",
    "train_land = train_data[train_data['class'] == 'land'].drop(['class', 'num'], axis=1)\n",
    "train_land = pd.DataFrame(scaler.fit_transform(train_land).round(5))\n",
    "\n",
    "train_imap = train_data[train_data['class'] == 'imap'].drop(['class', 'num'], axis=1)\n",
    "train_imap = pd.DataFrame(scaler.fit_transform(train_imap).round(5))\n",
    "\n",
    "train_warezmaster = train_data[train_data['class'] == 'warezmaster'].drop(['class', 'num'], axis=1)\n",
    "train_warezmaster = pd.DataFrame(scaler.fit_transform(train_warezmaster).round(5))\n",
    "\n",
    "train_loadmodule = train_data[train_data['class'] == 'loadmodule'].drop(['class', 'num'], axis=1)\n",
    "train_loadmodule = pd.DataFrame(scaler.fit_transform(train_loadmodule).round(5))\n",
    "\n",
    "train_spy = train_data[train_data['class'] == 'spy'].drop(['class', 'num'], axis=1)\n",
    "train_spy = pd.DataFrame(scaler.fit_transform(train_spy).round(5))\n",
    "\n",
    "train_perl = train_data[train_data['class'] == 'perl'].drop(['class', 'num'], axis=1)\n",
    "train_perl = pd.DataFrame(scaler.fit_transform(train_perl).round(5))\n",
    "\n",
    "total_len = len(train_nonself)\n",
    "\n",
    "# 打印每一类长度和比例\n",
    "print(f\"train_Total: {len(train_nonself)}\")\n",
    "print(f\"train_neptune: {len(train_neptune)} ({len(train_neptune) / total_len:.2%})\")\n",
    "print(f\"train_warezclient: {len(train_warezclient)} ({len(train_warezclient) / total_len:.2%})\")\n",
    "print(f\"train_ipsweep: {len(train_ipsweep)} ({len(train_ipsweep) / total_len:.2%})\")\n",
    "print(f\"train_portsweep: {len(train_portsweep)} ({len(train_portsweep) / total_len:.2%})\")\n",
    "print(f\"train_teardrop: {len(train_teardrop)} ({len(train_teardrop) / total_len:.2%})\")\n",
    "print(f\"train_nmap: {len(train_nmap)} ({len(train_nmap) / total_len:.2%})\")\n",
    "print(f\"train_guess_passwd: {len(train_guess_passwd)} ({len(train_guess_passwd) / total_len:.2%})\")\n",
    "print(f\"train_ftp_write: {len(train_ftp_write)} ({len(train_ftp_write) / total_len:.2%})\")\n",
    "print(f\"train_multihop: {len(train_multihop)} ({len(train_multihop) / total_len:.2%})\")\n",
    "print(f\"train_satan: {len(train_satan)} ({len(train_satan) / total_len:.2%})\")\n",
    "print(f\"train_smurf: {len(train_smurf)} ({len(train_smurf) / total_len:.2%})\")\n",
    "print(f\"train_pod: {len(train_pod)} ({len(train_pod) / total_len:.2%})\")\n",
    "print(f\"train_back: {len(train_back)} ({len(train_back) / total_len:.2%})\")\n",
    "print(f\"train_rootkit: {len(train_rootkit)} ({len(train_rootkit) / total_len:.2%})\")\n",
    "print(f\"train_buffer_overflow: {len(train_buffer_overflow)} ({len(train_buffer_overflow) / total_len:.2%})\")\n",
    "print(f\"train_phf: {len(train_phf)} ({len(train_phf) / total_len:.2%})\")\n",
    "print(f\"train_land: {len(train_land)} ({len(train_land) / total_len:.2%})\")\n",
    "print(f\"train_imap: {len(train_imap)} ({len(train_imap) / total_len:.2%})\")\n",
    "print(f\"train_warezmaster: {len(train_warezmaster)} ({len(train_warezmaster) / total_len:.2%})\")\n",
    "print(f\"train_loadmodule: {len(train_loadmodule)} ({len(train_loadmodule) / total_len:.2%})\")\n",
    "print(f\"train_spy: {len(train_spy)} ({len(train_spy) / total_len:.2%})\")\n",
    "print(f\"train_perl: {len(train_perl)} ({len(train_perl) / total_len:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_saint: 319\n",
      "test_mscan: 996\n",
      "test_apache2: 737\n",
      "test_snmpgetattack: 178\n",
      "test_processtable: 685\n",
      "test_httptunnel: 133\n",
      "test_ps: 15\n",
      "test_snmpguess: 331\n",
      "test_mailbomb: 293\n",
      "test_named: 17\n",
      "test_sendmail: 14\n",
      "test_xterm: 13\n",
      "test_worm: 2\n",
      "test_xlock: 9\n",
      "test_xsnoop: 4\n",
      "test_sqlattack: 2\n",
      "test_udpstorm: 2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 测试集未知数据攻击分类\n",
    "test_saint = test_data[test_data['class'] == 'saint'].drop(['class', 'num'], axis=1)\n",
    "test_saint = pd.DataFrame(scaler.fit_transform(test_saint).round(5))\n",
    "\n",
    "test_mscan = test_data[test_data['class'] == 'mscan'].drop(['class', 'num'], axis=1)\n",
    "test_mscan = pd.DataFrame(scaler.fit_transform(test_mscan).round(5))\n",
    "\n",
    "test_apache2 = test_data[test_data['class'] == 'apache2'].drop(['class', 'num'], axis=1)\n",
    "test_apache2 = pd.DataFrame(scaler.fit_transform(test_apache2).round(5))\n",
    "\n",
    "test_snmpgetattack = test_data[test_data['class'] == 'snmpgetattack'].drop(['class', 'num'], axis=1)\n",
    "test_snmpgetattack = pd.DataFrame(scaler.fit_transform(test_snmpgetattack).round(5))\n",
    "\n",
    "test_processtable = test_data[test_data['class'] == 'processtable'].drop(['class', 'num'], axis=1)\n",
    "test_processtable = pd.DataFrame(scaler.fit_transform(test_processtable).round(5))\n",
    "\n",
    "test_httptunnel = test_data[test_data['class'] == 'httptunnel'].drop(['class', 'num'], axis=1)\n",
    "test_httptunnel = pd.DataFrame(scaler.fit_transform(test_httptunnel).round(5))\n",
    "\n",
    "test_ps = test_data[test_data['class'] == 'ps'].drop(['class', 'num'], axis=1)\n",
    "test_ps = pd.DataFrame(scaler.fit_transform(test_ps).round(5))\n",
    "\n",
    "test_snmpguess = test_data[test_data['class'] == 'snmpguess'].drop(['class', 'num'], axis=1)\n",
    "test_snmpguess = pd.DataFrame(scaler.fit_transform(test_snmpguess).round(5))\n",
    "\n",
    "test_mailbomb = test_data[test_data['class'] == 'mailbomb'].drop(['class', 'num'], axis=1)\n",
    "test_mailbomb = pd.DataFrame(scaler.fit_transform(test_mailbomb).round(5))\n",
    "\n",
    "test_named = test_data[test_data['class'] == 'named'].drop(['class', 'num'], axis=1)\n",
    "test_named = pd.DataFrame(scaler.fit_transform(test_named).round(5))\n",
    "\n",
    "test_sendmail = test_data[test_data['class'] == 'sendmail'].drop(['class', 'num'], axis=1)\n",
    "test_sendmail = pd.DataFrame(scaler.fit_transform(test_sendmail).round(5))\n",
    "\n",
    "test_xterm = test_data[test_data['class'] == 'xterm'].drop(['class', 'num'], axis=1)\n",
    "test_xterm = pd.DataFrame(scaler.fit_transform(test_xterm).round(5))\n",
    "\n",
    "test_worm = test_data[test_data['class'] == 'worm'].drop(['class', 'num'], axis=1)\n",
    "test_worm = pd.DataFrame(scaler.fit_transform(test_worm).round(5))\n",
    "\n",
    "test_xlock = test_data[test_data['class'] == 'xlock'].drop(['class', 'num'], axis=1)\n",
    "test_xlock = pd.DataFrame(scaler.fit_transform(test_xlock).round(5))\n",
    "\n",
    "test_xsnoop = test_data[test_data['class'] == 'xsnoop'].drop(['class', 'num'], axis=1)\n",
    "test_xsnoop = pd.DataFrame(scaler.fit_transform(test_xsnoop).round(5))\n",
    "\n",
    "test_sqlattack = test_data[test_data['class'] == 'sqlattack'].drop(['class', 'num'], axis=1)\n",
    "test_sqlattack = pd.DataFrame(scaler.fit_transform(test_sqlattack).round(5))\n",
    "\n",
    "test_udpstorm = test_data[test_data['class'] == 'udpstorm'].drop(['class', 'num'], axis=1)\n",
    "test_udpstorm = pd.DataFrame(scaler.fit_transform(test_udpstorm).round(5))\n",
    "\n",
    "# 打印每一类长度\n",
    "print(f\"test_saint: {len(test_saint)}\")\n",
    "print(f\"test_mscan: {len(test_mscan)}\")\n",
    "print(f\"test_apache2: {len(test_apache2)}\")\n",
    "print(f\"test_snmpgetattack: {len(test_snmpgetattack)}\")\n",
    "print(f\"test_processtable: {len(test_processtable)}\")\n",
    "print(f\"test_httptunnel: {len(test_httptunnel)}\")\n",
    "print(f\"test_ps: {len(test_ps)}\")\n",
    "print(f\"test_snmpguess: {len(test_snmpguess)}\")\n",
    "print(f\"test_mailbomb: {len(test_mailbomb)}\")\n",
    "print(f\"test_named: {len(test_named)}\")\n",
    "print(f\"test_sendmail: {len(test_sendmail)}\")\n",
    "print(f\"test_xterm: {len(test_xterm)}\")\n",
    "print(f\"test_worm: {len(test_worm)}\")\n",
    "print(f\"test_xlock: {len(test_xlock)}\")\n",
    "print(f\"test_xsnoop: {len(test_xsnoop)}\")\n",
    "print(f\"test_sqlattack: {len(test_sqlattack)}\")\n",
    "print(f\"test_udpstorm: {len(test_udpstorm)}\")\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown: 3750\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 生成unknown_set\n",
    "unknown = pd.concat([test_saint, test_mscan, test_apache2, test_snmpgetattack, test_processtable, test_httptunnel, test_ps, test_snmpguess, test_mailbomb, test_named, test_sendmail, test_xterm, test_worm, test_xlock, test_xsnoop, test_sqlattack, test_udpstorm], axis=0)\n",
    "unknown.to_csv(\"unknown.csv\",index=False)\n",
    "\n",
    "print(f\"unknown: {len(unknown)}\")\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 1922\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 训练集采样\n",
    "import pandas as pd\n",
    "trainset_neptune_sampled = train_neptune.sample(n=1000, random_state=42)\n",
    "trainset_warezclient_sampled = train_warezclient.sample(n=50, random_state=42)\n",
    "trainset_ipsweep_sampled = train_ipsweep.sample(n=100, random_state=42)\n",
    "trainset_portsweep_sampled = train_portsweep.sample(n=100, random_state=42)\n",
    "trainset_teardrop_sampled = train_teardrop.sample(n=100, random_state=42)\n",
    "trainset_nmap_sampled = train_nmap.sample(n=50, random_state=42)\n",
    "trainset_guess_passwd_sampled = train_guess_passwd.sample(n=50, random_state=42)\n",
    "trainset_ftp_write_sampled = train_ftp_write\n",
    "trainset_multihop_sampled = train_multihop\n",
    "trainset_satan_sampled = train_satan.sample(n=100, random_state=42)\n",
    "trainset_smurf_sampled = train_smurf.sample(n=100, random_state=42)\n",
    "trainset_pod_sampled = train_pod.sample(n=50, random_state=42)\n",
    "trainset_back_sampled = train_back.sample(n=100, random_state=42)\n",
    "trainset_rootkit_sampled = train_rootkit\n",
    "trainset_buffer_overflow_sampled = train_buffer_overflow\n",
    "trainset_phf_sampled = train_phf\n",
    "trainset_land_sampled = train_land\n",
    "trainset_imap_sampled = train_imap\n",
    "trainset_warezmaster_sampled = train_warezmaster\n",
    "trainset_loadmodule_sampled = train_loadmodule\n",
    "trainset_spy_sampled = train_spy\n",
    "trainset_perl_sampled = train_perl\n",
    "\n",
    "seed = pd.concat([\n",
    "    trainset_neptune_sampled,\n",
    "    trainset_warezclient_sampled,\n",
    "    trainset_ipsweep_sampled, \n",
    "    trainset_portsweep_sampled,\n",
    "    trainset_teardrop_sampled,\n",
    "    trainset_nmap_sampled,\n",
    "    trainset_guess_passwd_sampled,\n",
    "    trainset_ftp_write_sampled,\n",
    "    trainset_multihop_sampled,\n",
    "    trainset_satan_sampled,\n",
    "    trainset_smurf_sampled,\n",
    "    trainset_pod_sampled,\n",
    "    trainset_back_sampled,\n",
    "    trainset_rootkit_sampled,\n",
    "    trainset_buffer_overflow_sampled,\n",
    "    trainset_phf_sampled,\n",
    "    trainset_land_sampled,\n",
    "    trainset_imap_sampled,\n",
    "    trainset_warezmaster_sampled,\n",
    "    trainset_loadmodule_sampled,\n",
    "    trainset_spy_sampled,\n",
    "    trainset_perl_sampled\n",
    "], axis=0)\n",
    "\n",
    "print(f\"seed: {len(seed)}\")\n",
    "# 打乱数据\n",
    "seed = seed.sample(frac=1, random_state=42)\n",
    "\n",
    "# 计算每组大小\n",
    "group_size = len(seed) // 7\n",
    "\n",
    "# 分成7组并保存\n",
    "for i in range(7):\n",
    "    start_idx = i * group_size\n",
    "    end_idx = start_idx + group_size if i < 6 else len(seed)\n",
    "    group = seed.iloc[start_idx:end_idx]\n",
    "    group.to_csv(f\"seed_{i+1}.csv\", index=False)\n",
    "    \n",
    "print('\\n')\n",
    "seed.to_csv(\"seed.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nonself.to_csv(\"train_nonself.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "train_nonself = pd.DataFrame(scaler.fit_transform(train_nonself).round(5))\n",
    "test_nonself = pd.DataFrame(scaler.fit_transform(test_nonself).round(5))\n",
    "train_self = pd.DataFrame(scaler.fit_transform(train_self).round(5))\n",
    "test_self = pd.DataFrame(scaler.fit_transform(test_self).round(5))\n",
    "\n",
    "test_nonself.to_csv(\"test_nonself.csv\",index=False)\n",
    "train_nonself.to_csv(\"train_nonself.csv\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_self_20 =  train_self.sample(n=56000, random_state=42)\n",
    "train_self_20.to_csv(\"evaluate/train_self.csv\",index = False)\n",
    "\n",
    "test_self_20 = test_self.sample(n=3000 , random_state=42)\n",
    "test_self_20.to_csv(\"evaluate/test_self.csv\",index=False)\n",
    "\n",
    "test_nonself = pd.read_csv(\"nonself/test_nonself.csv\")\n",
    "test_nonself = test_nonself.sample(n=3000, random_state=42)\n",
    "test_nonself.to_csv(\"evaluate/test_nonself.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
